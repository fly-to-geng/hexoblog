---
title: 机器学习_分类器性能的度量
toc: true

tags:
  - ML
date: 2017-06-03 13:18:13
---

分类器性能的度量指标，包括`accuracy`,`recall`,`Specificity`,`precision`,`f1`,`ROC`,等概念的定义和如何使用`matplotlib`绘制相应的图形。

<!-- more -->
导入需要用到的包

```python
from sklearn import metrics
```

对于二分类问题，我们用列表示真正的类别标签，用行表示分类器预测的类别标签。

||positive|negative|
|--|--|--|
|positive| TP | FP |
|negative| FN | TN |

两个大写字母表示的简写可以这样理解：T表示预测正确，F表示预测错误。P表示正例(例如病人的分类，病人是正例，健康的是负例。)， N表示负例。TP表示正确的预测为正例(可知该样本本身为正例。)，FP表示错误的预测为正例(可知该样本本身为负例)，FN表示错误的预测成为负例(可知该样本本身为正例)，TN表示正确的预测成为负例(可知该样本本身为负例).

```python
# get the table
cm = confusion_matrix(y_true,y_pred)
```

### Accuracy

中文一般翻译成**准确率**，这个是最容易理解的，就是分类正确的样本数量占样本总量的百分比。

$$
Accuracy = \dfrac {TP+TN} {TP+FP+TN+FN}
$$

```python
accuracy_scores = metrics.accuracy_score(y_true,y_pred)
```


### Sensitivity

**Sensitivity(灵敏度)**==**recall(召回率)**==**true positive rate(真阳性率)**==**probability of detection**

$$
recall = \dfrac {TP} {TP+FN}
$$

含义是，所有病人中被分类成病人的 占 所有病人的百分比。

```python
recall_scores=metrics.recall_score(y_true,y_pred)
```

### Specificity

**Specificity**==**true negative rate**

$$
Specificity = \dfrac {TN} {TN+FP}
$$

健康人被误分类成病人 占 所有健康人的 百分比。

```python
specificity_scores = cm[1][1] / (float)(cm[1][1]+cm[0][1])
```

### precision

**precision** == **positive predictive value**

$$
precision = \dfrac {TP} {TP+FP}
$$

在所有被分类成病人的样本中，真正的病人 占的百分比。

```python
precision_scores=metrics.precision_score(y_true,y_pred)
```

### F1

精确率和召回率的调和平均值。

$$
\dfrac 2 {F1}=\dfrac 1 {precision}+\dfrac 1 {recall}=\dfrac {2TP} {2TP+FP+FN}
$$

```python
f1_scores = metrics.f1_score(y_true,y_pred)
```

### ROC

一种度量分类器性能的标准，曲线下面的面积越大，表示分类器越好。

```python
def plot_roc(pfAcc):
        """
        Parameters
        ----------
        pfAcc : (n_classifier, ) of dict, in each dict, it is a dict which has the key 'y_true','y_pred','y_predproba'

        Returns
        --------

        """
        color=['r','b','g','y','k','#eeefff','#e44fff','#eee44f','#eeef22','#e99fff']
        for i,(key,value) in enumerate(pfAcc.iteritems()):
            y_true = np.asarray([x[0] for x in value['y_true']])
            y_pred_prob = np.asarray([x[0][1] for x in value['y_predprob']])
            false_positive_rate, recall, thresholds = roc_curve(y_true,y_pred_prob,pos_label=2)
            roc_auc = auc(false_positive_rate, recall)
            plt.plot(false_positive_rate, recall, color[i], label=key+',AUC = %0.2f' % roc_auc)
        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')
        plt.xlim([-0.05, 1.05])
        plt.ylim([-0.05, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.legend(loc="lower right")
        plt.legend().draggable()
        plt.show()
```
