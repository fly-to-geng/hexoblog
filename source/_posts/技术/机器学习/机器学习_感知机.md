---
title: 机器学习_感知机
toc: true

tags:
  - perception
date: 2017-05-19 14:40:18
---
感知机是神经网络的基本组成单元，这里有一篇文章是从零基础开始介绍感知机的，本文就是参考她实现的感知机。<https://www.zybuluo.com/hanbingtao/note/433855>

<!-- more-->

## 自己动手实现感知机

感知机是只有一层功能神经元的最简单的神经网络，它只能处理简单的线性分类。它的基本形式是：

![perception](2017-05-19_145930.png)

设 $x$ 表示输入向量，$y$ 表示输出，感知机其实就是实现了以下的函数的结构：

$$
y = f(w \times x + b)
$$

其中，$w$ 是一个和 $x$ 长度相同的向量，代表每个 $x_i$ 的权重，$b$ 是一个实数，表示偏置。$f(x)$ 是任意一个定义在实数域上的函数，输入是一个实数，输出是一个实数。

感知机的**学习**就是根据给出的 $x$ 和对应的 $y$, 求解出参数 $w$ 和偏置 $b$. 感知机的**预测**就是利用训练好的模型( $w$ 和 $b$ 已经确定)，给出 $x$, 求解 $y$.

开始的时候，$w$ 和 $b$ 都初始化为0，用python代码表示就是：

```python
def __init__(self, input_num, activator):
        '''
        初始化感知器，设置输入参数的个数，以及激活函数。
        激活函数的类型为double -> double
        '''
        self.activator = activator
        # 权重向量初始化为0
        self.weights = [0.0 for _ in range(input_num)]
        # 偏置项初始化为0
        self.bias = 0.0
```

其中，`input_num`表示向量 $x$ 包含多少个数字，`activator` 是上式中的 $f(x)$.

感知机的预测就是在已知 $w$ 和 $b$ 的情况下， 计算 $y$.

```python
def predict(self, input_vec):
        '''
        输入向量，输出感知器的计算结果
        '''
        # 把input_vec[x1,x2,x3...]和weights[w1,w2,w3,...]打包在一起
        # 变成[(x1,w1),(x2,w2),(x3,w3),...]
        # 然后利用map函数计算[x1*w1, x2*w2, x3*w3]
        # 最后利用reduce求和
        return self.activator(
            reduce(lambda a, b: a + b,
                   map(lambda (x, w): x * w,
                       zip(input_vec, self.weights))
                , 0.0) + self.bias)
```

感知机的训练就是在给出 $x$ 和 $y$ 的情况下，一步一步调整 $w$ 和 $b$ 的取值，使得输入和输出尽可能的匹配。这里直接给出参数调整的方法，至于为什么会是这样，为什么这样可以保证得到结果，可以看算法导论上相关的数学推导。

$$
w_i \leftarrow w_i + \triangle w_i \\
b \leftarrow b + \triangle b \\
其中 \\
\triangle w = \beta (t-y) x_i \\
\triangle b = \beta (t-y)
$$

$y$ 是感知机在当前的参数设置条件下(初始化的时候都是0) 的输出， $y$ 是训练数据实际的标签，$\beta$ 称为**学习率**，控制参数更新的程度大小。可以看到，参数学习的时候，感知机是一条一条的处理的训练数据的，每处理一条训练数据，参数就更新一次，先来看看权重更新的代码：

```python
def _update_weights(self, input_vec, output, label, rate):
        '''
        按照感知器规则更新权重
        '''
        # 把input_vec[x1,x2,x3,...]和weights[w1,w2,w3,...]打包在一起
        # 变成[(x1,w1),(x2,w2),(x3,w3),...]
        # 然后利用感知器规则更新权重
        delta = label - output
        self.weights = map(
            lambda (x, w): w + rate * delta * x,
            zip(input_vec, self.weights))
        # 更新bias
        self.bias += rate * delta
```

把所有的训练数据都循环处理一遍，就完成了一次**迭代**，通常，只处理一次参数是不能得到充分的学习的，训练的时候要根据模型的复杂程度进行多次迭代，每次迭代参数都会更新。

```python
def _one_iteration(self, input_vecs, labels, rate):
        '''
        一次迭代，把所有的训练数据过一遍
        '''
        # 把输入和输出打包在一起，成为样本的列表[(input_vec, label), ...]
        # 而每个训练样本是(input_vec, label)
        samples = zip(input_vecs, labels)
        # 对每个样本，按照感知器规则更新权重
        for (input_vec, label) in samples:
            # 计算感知器在当前权重下的输出
            output = self.predict(input_vec)
            # 更新权重
            self._update_weights(input_vec, output, label, rate)
```

上面的代码就是完成一次迭代的过程，首先用现有的感知机的参数，预测输入 $x$ 对应的输出 $y$, 然后调用上面的参数更新函数，更新参数。

我们这里用**迭代次数**控制算法的停止时间，得到训练的方法. 当然，实际当中也可以用误差小于某一个给定的值来作为算法终止的条件。为什么要有误差，而不是百分之百的匹配上呢? 这是因为，在机器学习当中，百分之百拟合了训练数据，往往意味着过拟合了，在测试数据并不一定表现好，而我们需要的是在测试集上表现好。

```python
def train(self, input_vecs, labels, iteration, rate):
        '''
        输入训练数据：一组向量、与每个向量对应的label；以及训练轮数、学习率
        '''
        for i in range(iteration):
            self._one_iteration(input_vecs, labels, rate)
```

一个简单的感知机的实现就是这样，实际使用的时候，还可以添加一些模型可视化的方法，例如输出模型参数 $w$ 和 $b$ 的方法，显示每次迭代过程的方法等等。下面是完整的代码。

```python
class Perceptron(object):
    def __init__(self, input_num, activator):
        '''
        初始化感知器，设置输入参数的个数，以及激活函数。
        激活函数的类型为double -> double
        '''
        self.activator = activator
        # 权重向量初始化为0
        self.weights = [0.0 for _ in range(input_num)]
        # 偏置项初始化为0
        self.bias = 0.0
    def __str__(self):
        '''
        打印学习到的权重、偏置项
        '''
        return 'weights\t:%s\nbias\t:%f\n' % (self.weights, self.bias)
    def predict(self, input_vec):
        '''
        输入向量，输出感知器的计算结果
        '''
        # 把input_vec[x1,x2,x3...]和weights[w1,w2,w3,...]打包在一起
        # 变成[(x1,w1),(x2,w2),(x3,w3),...]
        # 然后利用map函数计算[x1*w1, x2*w2, x3*w3]
        # 最后利用reduce求和
        return self.activator(
            reduce(lambda a, b: a + b,
                   map(lambda (x, w): x * w,
                       zip(input_vec, self.weights))
                , 0.0) + self.bias)
    def train(self, input_vecs, labels, iteration, rate):
        '''
        输入训练数据：一组向量、与每个向量对应的label；以及训练轮数、学习率
        '''
        for i in range(iteration):
            self._one_iteration(input_vecs, labels, rate)
    def _one_iteration(self, input_vecs, labels, rate):
        '''
        一次迭代，把所有的训练数据过一遍
        '''
        # 把输入和输出打包在一起，成为样本的列表[(input_vec, label), ...]
        # 而每个训练样本是(input_vec, label)
        samples = zip(input_vecs, labels)
        # 对每个样本，按照感知器规则更新权重
        for (input_vec, label) in samples:
            # 计算感知器在当前权重下的输出
            output = self.predict(input_vec)
            # 更新权重
            self._update_weights(input_vec, output, label, rate)
    def _update_weights(self, input_vec, output, label, rate):
        '''
        按照感知器规则更新权重
        '''
        # 把input_vec[x1,x2,x3,...]和weights[w1,w2,w3,...]打包在一起
        # 变成[(x1,w1),(x2,w2),(x3,w3),...]
        # 然后利用感知器规则更新权重
        delta = label - output
        self.weights = map(
            lambda (x, w): w + rate * delta * x,
            zip(input_vec, self.weights))
        # 更新bias
        self.bias += rate * delta
```

下面我们使用写好的代码实现一个计算`and`功能的感知机。

```python
def and_f(x):
    '''
    定义激活函数f
    '''
    return 1 if x > 0 else 0

if __name__ == '__main__':
    # 训练and感知器
    X = [[0,0],[0,1],[1,0],[1,1]]
    y = [0,0,0,1]
    and_perception = Perceptron(2,and_f)
    and_perception.train(X,y,10,0.1)
    # 打印训练获得的权重
    print and_perception
    # 测试
    print '1 and 1 = %d' % and_perception.predict([1, 1])
    print '0 and 0 = %d' % and_perception.predict([0, 0])
    print '1 and 0 = %d' % and_perception.predict([1, 0])
    print '0 and 1 = %d' % and_perception.predict([0, 1])
```

输出类似这样子：
```
weights :[0.2, 0.1]
bias    :-0.200000

1 and 1 = 1
0 and 0 = 0
1 and 0 = 0
0 and 1 = 0
```

从中我们可以看出，对于`and`的计算全部正确。我们从训练好的模型中可以提取出实现该`and`功能的函数。

$$
y = f(0.2x_1 + 0.1x_2 -0.2)
$$

$f(x)$ 就是上面代码自定义的函数，当 $x>0$ 时输出1，否则输出0.

## scikit-learn中的感知机

多数情况下我们不需要自己实现感知机，有很多已经写好的代码可以使用，scikit-learn中就实现了感知机。我们直接调用它的函数，看看会是什么结果。

```python
from sklearn import linear_model
lmper = linear_model.Perceptron(n_iter=10)
lmper.fit(X,y)
print(lmper.coef_)
print(lmper.intercept_)
print '1 and 1 = %d' % lmper.predict([1, 1])
print '0 and 0 = %d' % lmper.predict([0, 0])
print '1 and 0 = %d' % lmper.predict([1, 0])
print '0 and 1 = %d' % lmper.predict([0, 1])

## output
[[ 3.  2.]]
[-4.]
1 and 1 = 1
0 and 0 = 0
1 and 0 = 0
0 and 1 = 0
```

我们可以得到，`scikit-learn`得到的可以用来计算`and`的函数是：

$$
 y = f(3x_1+2x_2-4)
$$

虽然和我们自己实现的不一样，但是功能都是一样的。目前还没有明白激活函数在哪里设置，所以最终参数的不同可能和激活函数的设置有关。


## keras 实现感知机

```python
X = [[0,0],[0,1],[1,0],[1,1]]
y = [0,0,0,1]
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop
import numpy as np
X = np.array(X)
y = np.array(y)
model = Sequential()
model.add(Dense(1, activation='relu', input_shape=(2,)))
model.compile(loss='mean_squared_error',
            optimizer='SGD',
            metrics=['accuracy'])
print(model.summary())
history = model.fit(X, y,batch_size=128, epochs=1)
print(model.get_weights())
# 测试
print('1 and 1 = %d' % model.predict(np.array([1, 1]).reshape(1,2)))
print('0 and 0 = %d' % model.predict(np.array([0, 0]).reshape(1,2)))
print('1 and 0 = %d' % model.predict(np.array([1, 0]).reshape(1,2)))
print('0 and 1 = %d' % model.predict(np.array([0, 1]).reshape(1,2)))

# output
[array([[-1.21217406],
       [-1.16362488]], dtype=float32), array([ 0.], dtype=float32)]
1 and 1 = 0
0 and 0 = 0
1 and 0 = 0
0 and 1 = 0
```

使用神经网络训练的并没有达到最优的结果，可能和我们训练的样本过少有关系。我们同样得到了训练出来的感知机：

$$
y = f(-1.2121x_1-1.1636x_2+0)
$$

不过这里只有75%的准确率，现在还不清楚里面的原理。

## 感知机的扩展

上面我们介绍的感知机使用了如下的激活函数：

$$
f(x)=\begin{cases}
        0   &  \text{x>0}\\
        1   &  \text{x<=0}
      \end{cases}
$$

这导致我们的输出智能是`0`,`1`,我们可以改变一下这个函数，使得输出的值是连续的，这样就可以用来解决回归的问题了。例如，我们直接使用 $f(x)=x$ 来作为激活函数。这样，我们实现了下面这样的线性函数：

$$
 y = w * x + b
$$

```python
def fx(x):
    return x

X = [[5], [3], [8], [1.4], [10.1]]
y = [5500, 2300, 7600, 1800, 11400]
linear_perception = Perceptron(1,fx)
linear_perception.train(X,y,10,0.1)
print linear_perception
print 'Work 3.4 years, monthly salary = %.2f' % linear_perception.predict([3.4])
print 'Work 15 years, monthly salary = %.2f' % linear_perception.predict([15])
print 'Work 1.5 years, monthly salary = %.2f' % linear_perception.predict([1.5])
print 'Work 6.3 years, monthly salary = %.2f' % linear_perception.predict([6.3])

# output
weights :[765.63113396072]
bias    :-778.455331

Work 3.4 years, monthly salary = 1824.69
Work 15 years, monthly salary = 10706.01
Work 1.5 years, monthly salary = 369.99
Work 6.3 years, monthly salary = 4045.02
```

上面训练了一个简单的一次线性函数：

$$
y = 765x-778
$$

## 更新规则的推导(梯度下降法)

上面介绍感知机的时候，我们直接给出了参数更新的方法，但是没有说明这个方法是如何得出来的，现在我们给出推导的方法。

$$
w_i \leftarrow w_i + \triangle w_i \\
b \leftarrow b + \triangle b \\
其中 \\
\triangle w = \beta (t-y) x_i \\
\triangle b = \beta (t-y)
$$

设模型的输出是 $\hat{y} $, 真实的标签是 $y$, 那么我们训练模型的目的是使得 $\hat y$ 尽可能的接近 $y$, 那么，如何度量它们之间的接近程度呢？ 一个简单的想法就是使用平方和.
$$
E = \sum_{i=1}^{n}( {\hat y_i}-y_i )^2
$$

有了损失函数，接下来我们的目标就是使得该函数达到极小值，这样 $\hat y$和 $y$ 之间就尽可能接近了。

如何求解函数的极小值呢，我们可以求函数的导数，令导数等于0，这样我们就能找到它的极值点。不幸的是，并不是所有的函数我们都能求解出导数的，所以求导数的方法并没有什么普适性，计算机有强大的计算能力，所以遇到这类问题，基本上都使用数值计算的方法解决。

我们直到一个函数的梯度表示一个函数上升最快的方向，那么她的反方向就是下降最快的方向，而梯度的计算就是函数对每一个变量的偏导数，这个我们可以事先计算出来。然后随机初始化一个 $x$, 求解$f(x)$ 的值，然后朝着梯度下降的方向更新 $x$, 经过无数次迭代之后，我们就能找到函数的极小值点对应的那个 $x$.

$ \hat y = f(w*x+b)$ , 令$x_0 = 1,w_0=b$, 可以把它写成统一的形式$\hat y = f(w*x)$ , 这样我们最终的目标是求解 $w$, 所以我们把 $E$ 看作 $w$ 的函数，这样优化函数变形为：

$$
E(w) = \sum_{i=1}^n ({\hat y_i}-y_i)^2
$$

求偏导的时候2次方会有一个系数2，所以我们把函数乘以 $\dfrac 1 2$, 消除求导时候的系数，这样优化的目标函数变为：

$$
E(w) =\dfrac 1 2 \sum_{i=1}^n ({\hat y_i}-y_i)^2
$$

下面就是求这个函数对 $w$ 的梯度了。首先我们复习一下链式求导法则。

$$
\dfrac {\partial E(w)} {\partial w} = \dfrac {E(w)} {\partial \hat y} * \dfrac {\partial \hat y} {\partial w}
$$

根据以上的链式求导法则，我们可以求解得到目标函数的梯度：

$$
\Delta {E(w)} = - \sum_{i=1}^n (y^i-\hat y^i)x
$$

有了函数的梯度，就可以按照梯度的方向更新参数了。

$$
w_{new} = w_{old} + \beta  \sum_{i=1}^n (y^i-\hat y^i)x^i
$$

式子中，$y^i$ 是真实的标签值，$x$是训练的样本， $\beta$是学习率，也就是每次梯度下降的步长。这个步长的设置是一个技术活，太小的话，收敛太慢，太大的话，一不小心就过了，找不到最小的位置。

把上面感知机的参数更新规则拿过来对比：

$$
w_i \leftarrow w_i + \triangle w_i \\
b \leftarrow b + \triangle b \\
其中 \\
\triangle w = \beta (t-y) x_i \\
\triangle b = \beta (t-y)
$$

我们发现二者是完全一致的， 原来，感知机也是按照梯度下降的方向更新参数的。

需要注意的是，我们上面介绍的是最朴素的梯度下降法，实际应用中，经常使用随机梯度下降法，就是每次更新参数的时候，并不是把所有的训练数据都遍历一边再计算一次更新，而是每一个样本都计算一次更新，这样可能会导致有使参数的更新不是朝着下降的方向，但是实际经验证明，总体趋势上还是下降的，并且这样的方法计算开销小，还很有可能收敛速度更快。
