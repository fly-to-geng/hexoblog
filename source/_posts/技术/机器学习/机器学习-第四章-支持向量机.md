---
title: 机器学习-第四章-支持向量机
toc: true

tags:
  - ML
date: 2017-11-04 09:05:10
---

【支持向量机】【函数间隔】【几何间隔】【KTT】

<!-- more -->

## 模型

令 $\theta = (w_1,w_2,...,w_n,b)$, $x = (x_1,x_2,...,x_n,1)$,则有：
$$
\theta^T \cdot x = w_1x_1+x_2x_2+...+w_nx_n+b
$$

支持向量机要得到的分类决策函数是：

$$
h_\theta(x) = sign(\theta^T \cdot x)
$$

其中，$sign(x)$的定义是：

$$
sign(x) = \begin{cases}
          1, & x>=0 \\
          0, & x < 0 \\
          \end{cases}
$$

当$\theta^T >= 0$ 的时候，分类成正类，当$\theta^T < 0$的时候，分类成负类。支持向量机就是通过给定的训练数据求解出$\theta$.

## 策略

### 函数间隔

$\theta^T \cdot x = 0$ 可以确定一个超平面(n维空间中，n-1维的叫做超平面)，$\theta^T \cdot x > 0$ 的时候，我们定义类别的标签是`1`, $\theta^T \cdot x < 0$ 的时候，我们定义类别的标签是`-1`. 这样，$\theta^T \cdot x$的符号和标签`y`的符号是一致的。 $ y(\\theta^T \cdot x ) > 0 $ 表示分类正确， $y(\\theta^T \cdot x)<0 $ 表示分类错误. 而$|\theta^T \cdot x|$表示的是点`x`到分类超平面的距离，可以认为距离越远，分类的可信度越高。

点$(x_i,y_i)$函数间隔的定义为：

$$
\hat \gamma_i = y_i(\theta^T \cdot x_i)
$$

训练集合中所有样本的函数间隔定义为所有样本的函数间隔最小的那一个。

### 几何间隔

$\theta^T \cdot x = wx+b$, 其中$w = (w_1,w_2,...,w_n)$. 函数间隔有一个问题，就是成比例的改变$w$和$b$的值，函数间隔的大小就会成比例的变化。所以函数间隔并不适合用来求极值点。 改进的方法是限制$w$的取值，是$||w|| = 1$, 这样函数间隔就变成了几何间隔。

样本$(x_i,y_i)$到分类超平面$wx+b=0$的几何间隔是：

$$
\gamma_i = y_i(\frac w {||w||} \cdot x_i + \frac b {||w||})
$$

训练集的几何间隔取 训练集合中所有的样本点到分类超平面几何间隔的最小值。

**优化目标**

支持向量机的优化目标就是找到使得 几何间隔最大化的参数取值。这是一个约束优化问题，目标是$max(\gamma)$, 约束条件是$\gamma_i >= \gamma$, 含义是在保证某个点的几何间隔都大于等于$\gamma$的情况下，求最大的$\gamma$. 

在约束条件两边都乘以一个$||w||$, 等式的性质不变，现在约束变成 $y_i(w \cdot x_i + b) >= \hat \gamma$, 因为 $\gamma = \frac {\hat \gamma} \gamma$, 最优化的目标变为： $\frac {\hat \gamma} {||w||}$, $\hat \gamma$的取值是不影响最后的结果的，前面介绍过了，函数间隔会根据$w$和$b$的取值等比例的缩放，所以这里我们令$\hat \gamma = 1$, 最优化问题可以写成下面的形式：

$$
min \quad \frac 1 2 {||w||^2} \\
s.t. \quad y_i(w \cdot x_i + b)-1 >= 0, \quad \text{i=1,2,...,n}
$$

> 最大化 $\frac 1 {||w||}$ 等价于 最小化$\frac 1 2 {||w||^2}$

支持向量机就是求解上面的最优化问题，来求得参数$\theta$，然后构造分类超平面为未知的数据分类的。

## 方法

### 拉格朗日函数

凸优化问题的一般形式

$$
min \quad f(x) \\
s.t. \quad c_i(x) <= 0, \quad i = 1,2,...,k \\
           h_j(x) = 0, \quad j = 1,2,...,l
$$

其中，$f(x)$和$c_i(x)$ 都是 $R^n$上的连续可微的凸函数，$h_j(x)$ 是 $R^n$上的仿射函数。

> $f(x)$是仿射函数的含义是满足条件：$f(x) = ax+b$,其中$a \in R^n$,$b \in R$,$x \in R^n$.

拉格朗日函数是在 约束优化问题中 用来把约束条件添加到目标函数中的一种手段，广义的拉格朗日函数的定义是：

$$
L(x,\alpha,\beta) = f(x) + \sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x) \quad \alpha_i >= 0
$$

其中，$f(x)$是原来约束优化问题的目标函数，$c_i(x)$和$h_j(x)$是约束条件中的函数.

定义函数：

$$
P(x) = \mathop{max} \limits_{\alpha,\beta} L(x,\alpha,\beta)
$$

其中，$P(x)$表示在把$x$看作常数，$\alpha,\beta$看作变量的情况下，求$L(x,\alpha,\beta)$的最大值；

现在来分析函数$P(x)$, 如果$x$满足原来最优化问题的约束条件，即$c_i(x) <= 0$, $h_j(x) = 0$, 那么 $P(x) = \mathop{max} \limits_{\alpha,\beta} ( f(x) + \sum_{i=1}^k \alpha_i c_i(x))$, 这个时候的最大值的结果就是$P(x) = f(x)$, $\alpha_i = 0$. 除此之外，$\alpha_i$取任何大于0的值最后的结果都会减小。

如果$x$不满足约束条件，即 $c_i(x) > 0$ 或者 $h_j(x) \neq 0$, 这个时候总能找到满足的 $\alpha$ 或 $\beta$ ,使得 $P(x) = +\infty$ . 例如，如果$c_i(x) > 0$, 只需要让$\alpha_i$足够大，就能保证结果足够大，如果$h_j(x) \neq 0$, 只需要让$\beta_j$足够大就能保证结果趋近 $+\infty$ .

综上，$P(x)$是一个满足下述条件的函数：

$$
P(x) = \begin{cases}
        f(x) & \text{x 满足约束条件} \\
        +\infty & \text{x 不满足约束条件}
       \end{cases}
$$

那么， $min P(x)$ 表示求$P(x)$的最小值，其含义就是求在满足约束条件的情况下，$f(x)$的最小值。所以原来的约束最优化问题，在引入拉格朗日函数之后，可以变成下面的无约束优化问题，它与原来的问题等价：

$$
\mathop{min} \limits_x   \quad \mathop{max} \limits_{\alpha,\beta} ( f(x) + \sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x))  \quad \alpha_i >= 0
$$

### 对偶问题

如果原始问题是：

$$
p^*  =  \mathop{min} \limits_x   \quad \mathop{max} \limits_{\alpha,\beta} ( f(x) + \sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x))  \quad \alpha_i >= 0
$$

则它的对偶问题是：

$$
d^* =   \mathop{max} \limits_{\alpha,\beta}  \quad  \mathop{min} \limits_x   ( f(x) + \sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x))  \quad \alpha_i >= 0
$$

原问题是先固定的把$x$看作常数，求解$\alpha,\beta$, 再把$\alpha,\beta$看作常数，求解$x$;
而对偶问题正好反了过来，是先把$\alpha,\beta$看作常数，求解出$x$,再把$x$看作常数，求解出$\alpha,\beta$

如果$p^*$表示原始问题的最优解，而$d^*$表示对偶问题的最优解，则有$d^* <= p^*$. 原问题的解是大于等于对偶问题的最优解的，如果想通过解对偶问题来解原来的问题，需要满足如下的KKT条件，才能保证原问题和对偶问题的解相同：

$$
\triangledown_x L(x,\alpha,\beta) = 0 \\
\triangledown_{\alpha} L(x,\alpha,\beta) = 0 \\
\triangledown_{\beta} L(x,\alpha,\beta) = 0 \\
\alpha_ic_i(x) = 0,i=1,2,...,k \\
c_i(x) <=0, i = 1,2,...,k \\
\alpha_i >= 0, i=1,2,...,k \\
h_j(x) = 0, j=1,2,...,l
$$

其中， $x,\alpha,\beta$,分别代表原始问题和最优问题的解。如果满足上述的约束条件，则原始问题和对偶问题有相同的解。 前三个条件是对$x,\alpha,\beta$的偏导数为0， 后三个条件是原来问题的约束条件，只有中间的条件是新的约束条件 $\alpha_i c_i(x) = 0$, 这意味着如果$\alpha_i > 0$, 那么$c_i(x) = 0$，否则约束条件就无法得到满足。

### 求解原问题

支持向量机的优化目标是：

$$
min \quad \frac 1 2 {||w||^2} \\
s.t. \quad y_i(w \cdot x_i + b)-1 >= 0, \quad \text{i=1,2,...,n}
$$

把约束条件加入优化函数，改写成拉格朗日函数如下：

$$
L(w,\alpha) = \frac 1 2 {||w||^2} - ( \sum_{k=1}^m \alpha_i (y_i(w \cdot x_i + b)-1) )
$$

因为没有等式约束条件，所以这里没有$\beta$, 还有这里是$-$号，因为凸优化的标准约束是小于等于的形式，而这里却是大于等于的形式。

所以原来的优化问题可以转化成求解下面的问题：

$$
\mathop{min} \limits_w \quad \mathop{max} \limits_{\alpha} L(w,\alpha)
$$

它的对偶问题是:

$$
\mathop{max} \limits_{\alpha} \quad \mathop{min} \limits_w  L(w,\alpha)
$$

首先要求解$\mathop{min} \limits_w  L(w,\alpha)$, 这个时候$w,b$是变量，$\alpha$看作常量，求解函数 $\frac 1 2 {||w||^2} - ( \sum_{k=1}^m \alpha_i (y_i(w \cdot x_i + b)-1) )$ 的最小值。

> 多元函数的极值点必在驻点（偏导数为0的点）和偏导数不存在的点中。

要求极值点，首先对每个变量($w,b$)求偏导数，令其等于0：

$$
\triangledown_w L(w,b,\alpha) = w - \sum_{i=1}^m \alpha_i y_i x_i = 0 \\
\triangledown_b L(w,b,\alpha) = \sum_{i=1}^m \alpha_i y_i = 0
$$

把得到的等式带入原来的式子$L(w,b,\alpha)$中化简,去掉$w,b$,得到只有$x,y,\alpha$的表达式：

$$
\mathop{min} \limits_{w,b}  L(w,b,\alpha)  = - \frac 1 2 \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j(x_i \cdot  x_j) + \sum_{i=1}^m \alpha_i
$$

上式中，$w,b$已经确定了，得到了一个极小值，下一步是求极大值$\mathop{max} \limits_{\alpha} \quad \mathop{min} \limits_{w,b}  L(w,b,\alpha)$, 具体的式子带入，转化成求如下的约束优化问题：

$$
\mathop{max} \limits_{\alpha}  \quad - \frac 1 2 \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j(x_i \cdot  x_j) + \sum_{i=1}^m \alpha_i \\
s.t. \quad \sum_{i=1}^m \alpha_i y_i = 0 \\
    \alpha_i >= 0, \quad i=1,2,...,m
$$

把求解最大化问题转化成求解最小化问题：

$$
\mathop{min} \limits_{\alpha}  \quad  \frac 1 2 \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j(x_i \cdot  x_j) - \sum_{i=1}^m \alpha_i \\
s.t. \quad \sum_{i=1}^m \alpha_i y_i = 0 \\
    \alpha_i >= 0, \quad i=1,2,...,m
$$

设该问题的解是$\alpha^*$, 则原来问题的解是$w^*,b^*$, KKT条件成立，可以导出三者之间的关系为：

$$
w^* = \sum_{i=1}^m \alpha^* y_i x_i \\
b^* = y_i - \sum_{i=1}^m \alpha^* y_i (x_i \cdot x_j)
$$


### 求解支持向量机参数的一般步骤

输入：训练集$(x_i,y_i)$, 训练集合有样本`m`个，特征数量`n`个。

输出：分类决策函数

**最大间隔法**

1. 构造并求解约束最优化问题：

$$
\mathop{min} \limits_{w,b} \quad \frac 1 2 {||w||^2} \\
s.t. \quad  y_i(w \cdot x_i + b) - 1 >= 0, \quad i=1,2,...,m
$$

2. 求解该问题得到问题的解$w^*,b^*$

3. 构造分类平面 $w^* \cdot x + b^* = 0$

**对偶学习算法**

1. 构造并求解约束最优化问题：

$$
\mathop{min} \limits_{\alpha} \quad \frac 1 2 \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i=1}^n \\
s.t. \quad \sum_{i=1}^m \alpha_i y_i = 0 \\
    \alpha_i >= 0, \quad i=1,2,...,m
$$

求解上面的最优化问题得到 $\alpha^*$.

2. 计算 $w^*$ 和 $b^*$ :

$$
w^* = \sum_{i=1}^n \alpha^* y_i x_i \\
b^* = y_i - \sum_{i=1}^n \alpha^*_i y_i (x_i \cdot x_j)
$$

3. 根据第二步求解的参数构造分类决策函数：

$$
w^* \cdot x + b^* = 0
$$

### 一个具体的例子

有三个训练样本，每个样本有两个特征，$x_1 = (3,3), x_2 = (4,3), x_3 = (1,1)$, 求支持向量机的分类超平面。其中$x_1,x_2$是正样本，$x_3$是负样本。

1. 用最大间隔法求解

根据数据构造约束最优化问题：

$$
\mathop{min} \limits_{w,b} \quad \frac 1 2 (w_1^2+w_2^2) \\
s.t. \quad 3w_1 + 3w_2 + b >= 1 \\
           4w_1 + 4w_2 + b >= 1 \\
           -w_1 - w_2 - b >= 1
$$

求解此最优化问题得到结果：$w_1 = w_2 = \frac 1 2, b = -2$, 最后的分类超平面是:

$$
\frac 1 2 x^{(1)} + \frac 1 2 x^{(2)} - 2 = 0
$$

2. 用对偶学习算法求解：

根据数据构造约束最优化问题：

$$
\mathop{min} \limits_{\alpha} \quad \frac 1 2 (18\alpha_1^2 + 25\alpha_2^2 + 2\alpha_3^2 + 42 \alpha_1 \alpha_2 - 12 \alpha_1 \alpha_3 - 14 \alpha_2 \alpha_3) - (\alpha_1 + \alpha_2 + \alpha_3)  \\
s.t. \quad \alpha_1 + \alpha_2 - \alpha_3 = 0 \\
           \alpha_i >= 0, \quad i=1,2,3
$$

求解最优化问题得到结果：$\alpha_1 = \frac 1 4, \alpha_2 = 0, \alpha_3 = \frac 1 4$.

则可以得到 $w_1^* = = w_2^* = \frac 1 2$, $b^* = -2$.

最后的分类超平面为：

$$
\frac 1 2 x^{(1)} + \frac 1 2 x^{(2)} - 2 = 0
$$

比较最大间隔算法和对偶学习算法可以看出，对偶学习算法更容易求解，因为其约束条件变得更少，更容易得到不同变量之间的转换关系。