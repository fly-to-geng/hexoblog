---
title: 深度学习_反向传播算法及简单实例
toc: true

tags:
  - BP
date: 2017-04-19 10:22:48
---

反向传播算法入门资源索引：

<http://www.52nlp.cn/tag/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95python%E4%BB%A3%E7%A0%81>

<!-- more -->

一个简单的三层网络：

![](2017-04-19_191339.png)

下面我们以上面的简单的三层的神经网络说明BP算法的计算过程。

初始的输入参数是：

|$x_1$|$x_2$|$x_3$|$w_{14}$|$w_{15}$|$w_{24}$|$w_25$|$w_{34}$|$w_{35}$|$w_{46}$|$w_{56}$|$b_4$|$b_5$|$b_6$|
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|1|0|1|0.2|-0.3|0.4|0.1|-0.5|0.2|-0.3|-0.2|-0.4|0.2|0.1|

该神经网络接受一个三维的向量作为输入[x1,x2,x3],输出只有一个数字，这里设置为不是0，就是1.示例给出的数据[1,0,1]对应的类别是1.

定义好了网络的结构，给出了初始的网络参数的值( $w_{14}$|$w_{15}$|$w_{24}$|$w_25$|$w_{34}$|$w_{35}$|$w_{46}$|$w_{56}$|$b_4$|$b_5$|$b_6$| ),都是该神经网络的参数。现在的目标是使得该神经网络在输入是[1,0,1]时，输出是1,或者至少是十分接近1.

首先计算一下在初始化参数的情况下，神经网络的输出是什么。
神经元输出的计算过程。

![](2017-04-19_102750.png)

在这里，激活函数我们使用sigmiod函数：

$$
f(x) =  \dfrac 1 {1+e^{-x}}
$$

我们将第i个神经元在激活函数作用之前的输出记作的输出记作 $I_i$ ,在激活函数作用之后的值记 $O_i$
计算过程如下：
$$
I_4 = x_1*w_{14}+x_2*w_{24}+x_3*w_{34}+b_4 = 1*0.2+0*0.4+1*(-0.5)+(-0.4) = -0.7
$$

$$
I_5 = x_1*w_{15}+x_2*w_{25}+x_3*w_{35}+b_5 = 1*(-0.3)+0*0.1+1*0.2+0.2 = 0.1
$$

$$
I_6 = O_4*w_{46}+O_5*w_{56}+b_6 = 0.331812*(-0.3)+0.524979*(-0.2)+0.1 = -0.104539
$$


$$
O_4 = f(I_4) = \dfrac 1 {1+e^{-(-0.7)}} = 0.331812
$$

$$
O_5 = f(I_5) = \dfrac 1 {1+e^{-0.1}} = 0.524979
$$

$$
O_6 = f(I_6) = \dfrac 1 {1+e^{-(-0.104539)}} = 0.473889
$$

可以看到，初始化的参数对[1,0,1]的输出是0.473899，而我们期望的输出是1。 所以，必须使用某种办法调整参数，才可以达到我们的期望。

我们定义误差如下：

$$
E = \frac12 (target - O_6)^2
$$

用平方误差来度量输出和期望的输出之间的差距。那么接下来我们希望调整神经网络的参数，使得E尽快的缩小，这样我们就可以使神经网络尽快的达到我们期望的输出。首先考虑如何改变$w_{46}$才能使得误差减小。这里我求 $ \frac {\partial{E}} {\partial w_{46}}$ , 导数对应的方向函数上升下降的速度是最快的，我们使用偏导数找到最快的下降方向，按照这样的方式更新神经网络的权重参数和偏置参数。

$$
\frac {\partial E} {\partial w_{46}} = \frac {\partial E} {\partial O_6} * \frac {\partial O_6} {\partial I_6} * \frac {\partial I_6} {\partial w_{46}}
$$

上面应用了求导的链式法则，将对$w_{46}$的偏导数表示成了已知的数据。下面我们一步一步求出这个偏导数的表达式并得出参数的更新规则。

$$
E = \frac12 (target - O_6)^2
$$

$$
\frac {\partial E} {\partial O_6} = -\frac12*2(target-O_6) = O_6 - target = 0.473889 - 1 = -0.526111
$$

$$
O_6 = f(I_6) = \frac {1} {1+e^{-I_6}}
$$
$$
\frac {\partial O_6} {\partial I_6} {} = e^{-I_6}*(\frac {1} {1+e^{-I_6}})^{2} = O_6 * (1-O_6) = 0.473889 * (1-0.473889) = 0.249318
$$

$$
I_6 = O_4*w_{46}+O_5*w_{56}+b_6
$$

$$
\frac {\partial I_6} {\partial w_{46}} = O_4 = 0.331812
$$

综合上面的所有公式，可以推导出最终的参数更新公式是：
$$
\frac {\partial E} {\partial w_{46}} = (O_6 - target) * (O_6*(1-O_6))*O_4=(-0.526111)*0.249318*0.331812= -0.043523
$$

我们定义一个学习率$l$,表示参数更新的速度，这里设定为0.9；然后定义参数按照下面的方式更新：
$$
w^+_{46} = w_{46} - l * \frac {\partial E} {\partial w_{46}} = -0.3 - 0.9*(-0.043523) =  -0.260829
$$


下面总结一下上面提到的公式，需要注意的是，我这里举例的神经网络只有一个输出，所有偏导数和导数是一样的。但是通常的神经网络都有多个输出。在有多个输出的时候，我们定义E等于多个输出节点的E的和。这样，多个节点的输出就和单个节点的输出不太一样。我们定义输出节点的错误率为：
$$
E_i = O_i*(1-O_i)*(target-O_i)
$$

我们定义隐藏层的节点的错误率为：
$$
E_i = O_i*(1-O_i)*( \sum E_k*w_{jk})
$$

这样，每次参数的更新就可以写成下面的形式：
$$
\Delta w_{ij} = l * E_j * O_i
$$
偏置的更新定义如下：
$$
\Delta b_j = l * E_j
$$

不断的应用以上的公式更新权重参数和偏置，直到错误率到达一个很小的水平，停止训练。这个时候，神经网络对输入[1,0,1]的输出已经非常接近正确答案1了。

第一轮权重和偏置的更新是：
![](2017-04-19_213636.png)

了解了反向传播算法，就掌握了神经网络参数学习的方法，我们就可以写出自己的神经网络了。

```python

# -*- coding: utf-8 -*-

import numpy as np
# 全连接层实现类
class FullConnectedLayer(object):
    def __init__(self, input_size, output_size,
                 activator):
        '''
        构造函数
        input_size: 本层输入向量的维度
        output_size: 本层输出向量的维度
        activator: 激活函数
        '''
        self.input_size = input_size
        self.output_size = output_size
        self.activator = activator
        # 权重数组W
        self.W = np.random.uniform(-0.1, 0.1,
            (output_size, input_size))
        # 偏置项b
        self.b = np.zeros((output_size, 1))
        # 输出向量
        self.output = np.zeros((output_size, 1))
    def forward(self, input_array):
        '''
        前向计算
        input_array: 输入向量，维度必须等于input_size
        '''
        # 式2
        self.input = input_array
        self.output = self.activator.forward(
            np.dot(self.W, input_array) + self.b)
    def backward(self, delta_array):
        '''
        反向计算W和b的梯度
        delta_array: 从上一层传递过来的误差项
        '''
        # 式8
        self.delta = self.activator.backward(self.input) * np.dot(
            self.W.T, delta_array)
        self.W_grad = np.dot(delta_array, self.input.T)
        self.b_grad = delta_array
    def update(self, learning_rate):
        '''
        使用梯度下降算法更新权重
        '''
        self.W += learning_rate * self.W_grad
        self.b += learning_rate * self.b_grad

# Sigmoid激活函数类
class SigmoidActivator(object):
    def forward(self, weighted_input):
        return 1.0 / (1.0 + np.exp(-weighted_input))
    def backward(self, output):
        return output * (1 - output)
# 神经网络类
class Network(object):
    def __init__(self, layers):
        '''
        构造函数
        '''
        self.layers = []
        for i in range(len(layers) - 1):
            self.layers.append(
                FullConnectedLayer(
                    layers[i], layers[i+1],
                    SigmoidActivator()
                )
            )
    def predict(self, sample):
        '''
        使用神经网络实现预测
        sample: 输入样本
        '''
        output = sample
        for layer in self.layers:
            layer.forward(output)
            output = layer.output
        return output
    def train(self, labels, data_set, rate, epoch):
        '''
        训练函数
        labels: 样本标签
        data_set: 输入样本
        rate: 学习速率
        epoch: 训练轮数
        '''
        for i in range(epoch):
            for d in range(len(data_set)):
                self.train_one_sample(labels[d],
                    data_set[d], rate)
    def train_one_sample(self, label, sample, rate):
        self.predict(sample)
        self.calc_gradient(label)
        self.update_weight(rate)
    def calc_gradient(self, label):
        delta = self.layers[-1].activator.backward(
            self.layers[-1].output
        ) * (label - self.layers[-1].output)
        for layer in self.layers[::-1]:
            layer.backward(delta)
            delta = layer.delta
        return delta
    def update_weight(self, rate):
        for layer in self.layers:
            layer.update(rate)

def train_and_evaluate():
    last_error_ratio = 1.0
    epoch = 0
    train_data_set, train_labels = get_training_data_set()
    test_data_set, test_labels = get_test_data_set()
    network = Network([784, 300, 10])
    while True:
        epoch += 1
        network.train(train_labels, train_data_set, 0.3, 1)
        print('%s epoch %d finished' % (now(), epoch) )
        if epoch % 10 == 0:
            error_ratio = evaluate(network, test_data_set, test_labels)
            print('%s after epoch %d, error ratio is %f' % (now(), epoch, error_ratio) )
            if error_ratio > last_error_ratio:
                break
            else:
                last_error_ratio = error_ratio
if __name__ == '__main__':
    train_and_evaluate()

```
