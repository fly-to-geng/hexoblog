---
title: 机器学习_算法汇总
toc: true

tags:
  - ML
date: 2017-05-15 21:49:51
---

机器学习是人工智能的一个分支,机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法.机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。本文主要介绍机器学习的基础算法和一些适用的问题。

<!-- more -->

机器学习的应用领域：

![](QQ截图20170515230814.png)

# 监督学习

监督学习是给出**特征**和特征对应的**标签**，让算法学习其中蕴含的规律。利用训练好的模型预测新的特征的标签的一类算法的统称。监督学习是最常见的机器学习算法，也是发展最成熟的一类。

## 线性模型(linear model)

线性模型试图学得一个通过属性的线性组合来进行预测的函数：

$$
 f(x)=w_1 x_1+x_2 x_2 + ... + w_d x_d + b
$$

线性模型中的$w$直观的表达了各个属性在预测过程中的重要性，所以线性模型有很好的可解释性。

如果在线性模型基础上，在输出之前用一个非线性的函数处理一下，就能得到非线性的决策边界，这样的模型叫做**广义线性模型**. 她有下面这样的形式：

$$
y = f(wx+b)
$$

其中，$f(x)$ 是非线性的函数。当他是`sigmoid`函数的时候，这个模型叫做**逻辑回归**,当它是`softmax`函数的时候，这个模型叫做**softmax**回归。

### 逻辑回归

Logistic回归优点：

　　1、实现简单；

　　2、分类时计算量非常小，速度很快，存储资源低；

缺点：

　　1、容易欠拟合，一般准确度不太高

　　2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；

### softmax回归

### 线性判别分析

## 决策树

根据树的结构进行决策，主要算法有ID3，C4.5.

> - 决策树的优点：计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；
>
> - 缺点：容易过拟合（后续出现了随机森林，减小了过拟合现象）.

## K近邻

KNN算法的优点：

　　1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；

　　2. 可用于非线性分类；

　　3. 训练时间复杂度为O(n)；

　　4. 准确度高，对数据没有假设，对outlier不敏感；

缺点：

　　1. 计算量大；

　　2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；

　　3. 需要大量的内存；

## 支持向量机

> - SVM算法优点：
>   -- 可用于线性/非线性分类，也可以用于回归；
    - 低泛化误差；
    - 容易解释；
    - 计算复杂度较低；

> - 缺点：

　　对参数和核函数的选择比较敏感；

　　原始的SVM只比较擅长处理二分类问题；


## 相关向量机

## 贝叶斯方法

### 朴素贝叶斯

### 半朴素贝叶斯

### 贝叶斯网络

### EM算法

## 神经网络

### 感知机

感知机是一种最简单的神经网络，单层神经网络。

### 径向基函数网络(Radial Basis Function)

一种单隐层的前馈神经网络，使用径向基函数作为隐层神经元激活函数，输出层是对隐层神经元的线性组合。

### 竞争型学习网络(competitive learning)

### 自组织映射网络(Self-Organizing Map)

### Elman网络

Elman是最常用的递归神经网络之一。

### Boltzmann机

一种基于能量概念的模型。

### 卷积神经网络(CNN)

### 深度信念网络(DBN)

### 循环神经网络(RNN)

### 稀疏自编码(SAE)


## 集成学习(ensemble learning)

通过构建并结合多个分类器来完成学习任务。

### Boosting方法

一类可以将若分类器提升为强分类器的算法。

**adaboost**

**Gradient Boosted Decision Tree**

### 随机森林(Random Forest)

# 半监督学习

半监督学习是介于有监督学习和无监督学习之间的，它是指训练样本中只有少量是有标签的数据，大量数据是都是无标签的，要从这样的数据中学习出数据内在的规律，从而能够把哪些无标签的数据也推断出一个合理的标签。半监督学习几乎是最符合实际情况的一类算法，因为在实际中，很多时候都只有少量的有标签的数据，大量的都是无标签的数据。所以，半监督学习方法的研究对实际应用很重要。

半监督学习之所以可行，是基于一个基本的假设：相似的样本又有相似的输出。如果我们拥有少量的带标记的样本，恰巧这些样本均匀分布在每个类别中，这样我们就可以先使用聚类来确定哪些样本是相似的，然后从这些相似的样本中找出一个带标签的，所有的这一簇样本就都属于这个标签。这就是半监督学习的基本思想。当然，度量样本相似性的方法不止**聚类**一种，还有一种是**流行假设**(manifold assumption).

**主动学习**，**纯半监督学习** 和 **直推式学习**

三种方法都是针对的都是 *训练集中少量样本是标记数据，大量样本是未标记数据* 这种情况，不同是，**主动学习**(active learning)是指首先用已经标记的数据训练一个模型，然后使用这个模型去预测未标记的数据，把预测出来的数据和*专家*给出的标记结果做比较，更新模型，使得模型越来越好。可以看到，本质上，它还是利用有标记的数据进行学习的。 **纯半监督学习** 是指用训练集训练模型，用测试集测试效果，训练集是包含有标记数据和未标记数据的。 **直推式学习** 是指学习到的模型要预测的数据就是训练集中未标记的数据，只需要把训练集中未标记的数据预测准确就好，不需要再考虑其他的未标记数据。

### 生成式方法(generative methods)

这类方法假定所有的数据都是基于一个*模型*生成的。此类方法简单，易于实现，而且效果不错，但是，这必须保证之前的假设是正确的。如果假设是错误的，该方法会得到很差的结果。遗憾的是，现实的世界中的数据，你很难准确的知道它是基于什么样的模型生成的。也许根本就不是基于同一个模型生成的。

### 半监督支持向量机(semi-supervised support vector machine)

这是在支持向量机的算法基础上进行扩展得到的。它的基本思想是：寻找能把有标记的数据区分开，并且穿过数据的低密度区域的分类超平面。这显然是基于我们上面提到过的聚类假设，假设相似的样本会聚集在一起。这类方法有很多，例如***S3VM**、**S4VM**、**CS4VM**、**TSVM**；

### 图论方法(graph-based methods)

我们把训练样本中每个样本对应图中的一个节点，而节点之间的边表示两个样本之间的相似性(相似性越强，边的权值越大)，我们把已经有标记的节点想象成已经染过色，而没有标记的节点还没有染色，这样问题转化成了在给定的图模型上为节点染色的问题。这类问题主要借助一类叫做**标记传播算法**(label propagation)的方法解决。

图论方法思路清晰，计算上可以有各种基于矩阵乘法的优化，但是其缺点很明显：

1. 存储开销大，不适合处理大数据。
2. 构图过程只考虑训练集，测试集中的样本到来的时候，难以判断其在图中的位置。

### 基于分歧的方法(disagreement-based methods)

**多视图数据**(multi-view data)

视图可能是从数据库中的术语借鉴过来的，一个视图就是一个属性的几何。我们可以把视图理解为某一个大的方面。例如针对电影数据来说，图像是一个视图，声音是一个视图，网上所有关于该电影的讨论也可以构成一个视图。把这些数据都收集起来放在一起，就构成了多视图数据。

**协同训练**(co-training)

协同训练是利用数据的多视图来进行训练的一类算法，它的基本假设是多视图数据的*相容互补性*。 *相容*是指通过任何一个视图推测出来的标签都应该是一样的，是匹配的。一个动作电影，只通过声音，只通过画面，都可以推断出是一个动作片(当然这不一定是真的)。  *互补* 是指通过一个视图的数据能够帮助另外一个视图的数据决定其标签。协同训练的过程是这样的：首先在每个视图数据上基于有标签的数据训练一个模型。然后，让每个模型去没有标记的数据中挑选自己最有把握分类的数据预测其标签，把预测出来标签的数据当作有标签的数据送给其他的分类器继续更新模型的参数，这样"互相学习，共同进步"，直到在所有视图数据上训练的模型都趋于稳定之后结束。

### 半监督聚类(semi-supervised clustering)

聚类虽然是一种典型的无监督算法，但是如果实现有一些有标记的数据，很有可能提高聚类算法的效果。例如，直到哪些样本之间必然属于同一类，直到哪些样本根本不可能在一起，或者直接直到某个样本属于什么类别。 这类方法主要是在聚类算法的基础上加上相应的约束实现的，主要有**约束K均值**(constrained k-means) **约束种子K均值**(constrained seed k-means).


# 无监督学习

无监督学习的含义是训练数据不包含任何标签，既然不包含任何标签，那么无论如何也学习不到数据内部和标签的对应关系(没有标签嘛，怎么学得到)。所以，无监督学习主要是用来发现数据自身的内部规律，例如，哪些数据和哪些数据比较相似。无监督学习的应用主要在**聚类**上.

## Hierarchical methods

### BIRCH(Balanced Iterative Reducing and Clustering Using Hierarchies)

### ROCK(A Hierarchical Clustering Algorithm for Categorical Attributes)

### Chameleon(A Hierarchical Clustering Algorithm Using Dynamic Modeling)


## Partition-based methods

### k-means

### k-means++

### kernel k-means

### k-medoids

### k-medians


## Density-based methods

### DBSCAN(Density-Based Spatial Clustering of Applications with Noise)

### OPTICS(Ordering Points To Identify Clustering Structure)

### DENCLUE

### WaveCluster

## Grid-based methods

### STING(Statistical Information Grid)

### CLIQUE(Clustering In Quest)

## Model-based methods

### GMM(Gaussian Mixture Models)

### SOM(Self Organized Maps)



# 强化学习

 强化学习对于包含长期反馈的问题比短期反馈的表现更好。它在许多问题上得到应用，包括机器人控制、电梯调度、电信通讯、双陆棋和西洋跳棋。

## Q Learning

## Sarsa

## Policy Gradients

## Actor-Critic

## Monte-Carlo Learning

## Deep Q network



# 规则学习(rule-base learning)



规则学习是最早开始研究的一类机器学习方法，它的基本思想是从数据中学习到一些基本的规则，用这些规则作用在新的数据上，决定新数据对应的输出。规则学习是*白箱模型*，每一步决策都会清晰的展现出来，算法学习得到的规则可以直接输出出来，对于一些简单的问题，我们甚至可以根据经验判断它学习的对不对。规则学习是最容易融合**专家知识**的机器学习方法。

规则学习的基本框架是**序贯覆盖**，**CN2** 采用集束搜索，是最早考虑过拟合问题的规则学习算法，**归纳逻辑程序设计**(ILP) 成为机器学习和知识工程的重要桥梁。

机器学习方法一开始是规则学习的天下，各种专家系统层出不穷，后来，统计学习方法逐渐占了上风，但是由于统计学习方法普遍都是黑箱模型，很难解释里面的原因，我们并不能直接从中学习得到知识。而且，在富含**结构信息**和**领域知识**的任务中，规则学习往往更适合。所以，未来的发展趋势是**基于规则的学习方法** 和 **统计学习方法** 的结合，使得我们的算法既有强大的学习能力，又能够教会我们数据中蕴含的**知识**，目前，这方便已经有了一些有益的探索。例如 **概率归纳逻辑程序设计**,**关系贝叶斯网**,**贝叶斯逻辑程序**,**马尔科夫逻辑网**.

# 其他算法

## 数据挖掘

### Apriori

### FP Growth:

## FFM+GDBT+LR

## 神经风格迁移

## genetic algorithm

## 模拟退火算法(Simulated annealing)

## 禁忌搜索(Tabu Search)

## 蚁群算法(Ant Colony Optimization)

## 粒子群优化(Particle Swarm Optimization)

## 引力搜索算法(GSA)

## 人工免疫算法

![](MachineLearningAlgorithms.png)
p
scikit-learn 实现的机器学习算法:

![](ml_map.png)

## 一个PPT

[机器学习算法汇总](https://1drv.ms/p/s!AsOuy4DDdLyCglfNtqwGpHPShTFo)
