---
title: 使用3D卷积神经神经网络提取脑成像数据的特征
toc: true

tags:
  - CNN
  - feature extraction
  - 特征提取
date: 2017-05-11 19:42:49
---
3D 卷积神经网络可以捕获3D空间的约束信息，利用它可以将原始的fMRI数据转换到另外的特征空间。
<!--more-->

```python
# -*- coding: utf-8 -*-
"""
Created on Thu May 11 19:40:32 2017

@author: FF120
"""
import os
import nibabel as nib
import nilearn
import numpy as np
#############################################################################
## 加载神经影像数据
#############################################################################
data_root = r'D:\FMRI_ROOT\YIYU\4D_data\funImg'
os.chdir(data_root)
lists_files = os.listdir()

subjects = []
subjects_data = []
for i in range(len(lists_files)):
    img = nib.load(lists_files[i])
    subjects.append(img)
    subjects_data.append(img.get_data())

#############################################################################
## 配置autoencoder的结构
## input_dim --> encoding_dim --> input_dim
#############################################################################
from keras.layers import Input, Dense
from keras.models import Model

# 中间隐藏层的 神经元 数目
encoding_dim = 150
#  输入神经元的数目
input_dim = 125
# 输入层神经元的数目
input_img = Input(shape=(input_dim,))
# "encoded" is the encoded representation of the input
encoded = Dense(encoding_dim, activation='relu')(input_img)
# "decoded" is the lossy reconstruction of the input
decoded = Dense(input_dim, activation='sigmoid')(encoded)

# this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

# this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

# create a placeholder for an encoded (32-dimensional) input
encoded_input = Input(shape=(encoding_dim,))
# retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]
# create the decoder model
decoder = Model(encoded_input, decoder_layer(encoded_input))

# 准备训练和测试用的数据
# 卷积核的大小
kernel_size = 5
subject0_0 = subjects_data[0][:,:,:,0]
# 每个scan取100个样本
sample_num = 100
list_kernels = []
for subject in range(len(subjects_data)):
    subject_data = subjects_data[subject]
    for scan in range(subject_data.shape[3]):
        scan_data = subject_data[:,:,:,scan]
        x_ = (np.random.random(size=sample_num)*(64-kernel_size)).astype(int)
        y_ = (np.random.random(size=sample_num)*(64-kernel_size)).astype(int)
        z_ = (np.random.random(size=sample_num)*(36-kernel_size)).astype(int)
        for i in range(sample_num):
            kernel_one = scan_data[x_[i]:(x_[i]+kernel_size),y_[i]:(y_[i]+kernel_size),z_[i]:(z_[i]+kernel_size)]
            list_kernels.append(kernel_one)


matrix_kernel = np.array(list_kernels)
matrix_kernel = matrix_kernel.reshape((len(matrix_kernel), np.prod(matrix_kernel.shape[1:])))
# 共抽取出来55*240*100 = 1320000个5*5*5的kernel，这个作为autoencoder的学习数据


autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')
autoencoder.fit(matrix_kernel, matrix_kernel,
                epochs=500,
                batch_size=100,
                shuffle=True)
# 保存模型
# 保存模型
def save_model(model,name):
    ## fit完成之后，保存整个模型的配置
    root_path = r'd:\deeplearning'
    model_config = model.get_config()
    model_weight = model.get_weights()
    import pickle
    import os
    config_save_path = os.path.join(root_path,name+"_config.txt")
    weight_save_path = os.path.join(root_path,name+"_weights.txt")
    pickle.dump(model_config, open(config_save_path, 'wb'))
    pickle.dump(model_weight, open(weight_save_path, 'wb'))

from models import model_from_json
def load_model(name):
    root_path = r'd:\deeplearning'
    import pickle
    import os
    config_save_path = os.path.join(root_path,name+"_config.txt")
    weight_save_path = os.path.join(root_path,name+"_weights.txt")
    model_config = pickle.load(open(config_save_path, 'rb'))
    model_weight = pickle.load(open(weight_save_path, 'rb'))
    model = Model.from_config(model_config)
    model.set_weights(model_weight)
    return model
model_save_path = r''
save_model(autoencoder,'3-layer-autoencoder-for-mdd')
## 获取训练好的模型的参数作为新的卷积网络的卷积核
autoencoder = load_model('3-layer-autoencoder-for-mdd')
weights = autoencoder.get_weights()[0]
list_kernel_weights = []
for i in range(weights.shape[1]):
    list_kernel_weights.append(weights[:,i].reshape(kernel_size,kernel_size,kernel_size))


## 使用autoencoder初始化卷积核已经成功，下面开始使用3D卷积为图像降维，最终生成一个降维之后的特征

# 卷积操作

import numpy as np
import tensorflow as tf

# 一幅图像一幅图像的卷积和池化
def convM(one_scan,kernels):
    graph1 = tf.Graph()
    with graph1.as_default():
        #inputs = [batch, in_depth, in_height, in_width, in_channels]
        #filter =  [filter_depth, filter_height, filter_width, in_channels,
        #  out_channels]
        # reshape成需要的输入格式
        f=tf.constant( one_scan.reshape(1,64,64,36,1) )
        g=tf.constant(  kernels.reshape(5,5,5,1,150)   )
        # 3D 卷积操作
        conv3=tf.nn.conv3d( f,g, strides=[1,1,1,1,1] , padding="VALID",name="conv1")
        # 池化操作
        pooling3 = tf.nn.max_pool3d(conv3, ksize=[1,1,1,1,1], strides=[1,5,5,5,1], padding='VALID', name=None)
    with tf.Session(graph=graph1) as sess:
        sess.run(tf.global_variables_initializer())
        result = (sess.run(pooling3))
        return result
        sess.close()

one_subject = subjects_data[0].astype(np.float32)
one_scan = one_subject[0:,0:,0:,0].astype(np.float32)
#   150 个 卷积核
kernels = weights.reshape(5,5,5,150).astype(np.float32)

## 使用循环处理每个被试和每个scan
re_list_subjects=[]
#num_su = len(subjects_data)
#num_sc = su.shape[3]
for subject in range( len(subjects_data) ):
    su = subjects_data[i]
    re_list_scan = []
    for scan in range( su.shape[3] ):
        sc = su[0:,0:,0:,scan].astype(np.float32)
        re_sc = convM(sc,kernels)
        re_list_scan.append(re_sc)

    re_list_subjects.append(re_list_scan)

feature_mat = np.asarray(re_list_subjects)
os.chdir(r'D:\deeplearning')
np.save('3d_pool_feature_result',feature_mat)
###得到卷积和池化之后的特征，处理成以后分类要用到的数据格式，并保存下来
re_feature=[]
for subject in re_list_subjects:
    for scan in subject:
        re_feature.append(scan.reshape(-1))

feature_2d = np.asarray(re_feature)
np.save('feature_2d_from_3d_pool',feature_mat)


## 使用一种降维方法，使用svm完成分类


```
