<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    
    <title>深度学习_Scikit-Learn机器学习算法的使用 | FEI&#39;s Blog</title>
    
    
        <meta name="keywords" content="python,scikit-learn">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="scikit-learn是一个很受欢迎的机器学习方面的python工具包，它定义的一些范式和处理流程影响深远，所以，认识和了解一些这个工具包对于自己实现一些机器学习算法是很有帮助的。它已经实现了很多方法帮助我们便捷的处理数据，例如，划分数据集为训练集和验证集，交叉验证，数据预处理，归一化等等。">
<meta name="keywords" content="python,scikit-learn">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习_Scikit-Learn机器学习算法的使用">
<meta property="og:url" content="http://ff120.github.io/hexoblog/2017/05/14/技术/机器学习/机器学习_Scikit-Learn使用技巧/index.html">
<meta property="og:site_name" content="FEI&#39;s Blog">
<meta property="og:description" content="scikit-learn是一个很受欢迎的机器学习方面的python工具包，它定义的一些范式和处理流程影响深远，所以，认识和了解一些这个工具包对于自己实现一些机器学习算法是很有帮助的。它已经实现了很多方法帮助我们便捷的处理数据，例如，划分数据集为训练集和验证集，交叉验证，数据预处理，归一化等等。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-03-28T00:54:57.903Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习_Scikit-Learn机器学习算法的使用">
<meta name="twitter:description" content="scikit-learn是一个很受欢迎的机器学习方面的python工具包，它定义的一些范式和处理流程影响深远，所以，认识和了解一些这个工具包对于自己实现一些机器学习算法是很有帮助的。它已经实现了很多方法帮助我们便捷的处理数据，例如，划分数据集为训练集和验证集，交叉验证，数据预处理，归一化等等。">
    

    
        <link rel="alternate" href="/atom.xml" title="FEI&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/hexoblog/favicon.ico">
    

    <link rel="stylesheet" href="/hexoblog/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/hexoblog/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/hexoblog/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/hexoblog/css/style.css">
    <script src="/hexoblog/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/hexoblog/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/hexoblog/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/hexoblog/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/hexoblog/" id="logo">
                <i class="logo"></i>
                <span class="site-title">FEI&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/hexoblog/">首页</a>
                
                    <a class="main-nav-link" href="/hexoblog/archives">归档</a>
                
                    <a class="main-nav-link" href="/hexoblog/categories">分类</a>
                
                    <a class="main-nav-link" href="/hexoblog/tags">标签</a>
                
                    <a class="main-nav-link" href="/hexoblog/about">关于</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Rechercher">
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Articles',
            PAGES: 'Pages',
            CATEGORIES: 'Catégories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/hexoblog/',
        CONTENT_URL: '/hexoblog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/hexoblog/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/hexoblog/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/hexoblog/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/hexoblog/categories">分类</a></td>
                
                    <td><a class="main-nav-link" href="/hexoblog/tags">标签</a></td>
                
                    <td><a class="main-nav-link" href="/hexoblog/about">关于</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Rechercher">
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id="categories">
        <h3 class="widget-title">
            <span>Catégories</span>
            &nbsp;
            <a id="allExpand" href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            心理学
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            记忆魔法
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2019/04/03/心理学/记忆魔法/代码记忆法/">代码记忆法</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            技术
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Web开发
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_How-to-install-Laravel-framework/">How to install Laravel framework</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_laravel-4-note-01/">laravel 4 note 01</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_Make-phpStorm-friendly-to-laravel/">Make phpStorm friendly to laravel</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_sublime-Text-tricks/">sublime Text tricks</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_think-php-note-01/">think php note 01</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_think-php-note-02/">think php note 02</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_think-php-note-03/">think php note 03</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_PHP编译less文件-lessphp的使用/">PHP编译less文件-lessphp的使用</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_Lavarel-后台组件frozenode的使用/">Lavarel 后台组件frozenode的使用</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_Linux常用命令/">Linux常用命令</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_Linux主机之间同步文件/">Linux主机之间同步文件</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_PHP基本操作/">PHP基本操作</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_短信验证码的实现/">短信验证码的实现</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_配置Apache支持使用HTTPS/">配置Apache支持使用HTTPS</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/测试_使用Selenium测试UI/">测试_使用Selenium测试UI</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/后台开发_PhpStorm常用快捷键/">PhpStorm常用快捷键</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/微信开发_微信发送消息PHP-SDK/">微信发送消息PHP SDK</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/微信开发_获取地理位置/">微信获取地理位置 </a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/微信开发_发送模板消息的代码/">微信发送模板消息的代码</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/测试_Selenium-定位元素的几种方式/">测试_Selenium定位元素的几种方式</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/测试_Selenium-Action/">测试_Selenium Action</a></li>  <li class="file"><a href="/hexoblog/2016/06/11/技术/Web开发/测试_Apache-JMeter的使用/">测试_Apache JMeter的使用</a></li>  <li class="file"><a href="/hexoblog/2016/06/12/技术/Web开发/后台开发_Apache-配置虚拟主机/">Apache 配置虚拟主机</a></li>  <li class="file"><a href="/hexoblog/2016/06/23/技术/Web开发/后台开发_改进PHP的var-dump-方法使之适应显示从数据库中查出来的数据/">改进PHP的var_dump()方法使之适应显示从数据库中查出来的数据</a></li>  <li class="file"><a href="/hexoblog/2016/06/23/技术/Web开发/后台开发_PHP读写XLS/">PHP读写XLS</a></li>  <li class="file"><a href="/hexoblog/2016/06/30/技术/Web开发/前端_jQuery-EasyUI-学习笔记/">JQuery EasyUI 学习笔记</a></li>  <li class="file"><a href="/hexoblog/2016/06/30/技术/Web开发/前端_bootsharp学习笔记/">Bootsharp学习笔记</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            专业术语
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/07/24/技术/专业术语/术语/">英语</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            大数据
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2016/06/23/技术/大数据/大数据_Spark环境下的Kmeans-Python实现/">Spark环境下的Kmeans-Python实现</a></li>  <li class="file"><a href="/hexoblog/2017/10/06/技术/大数据/大数据基础框架/">大数据基础框架</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            工具
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            AutoHotKey
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2018/12/02/技术/工具/AutoHotKey/AutoHotKey非常有用的脚本/">AutoHotKey非常有用的脚本</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Git
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/05/04/技术/工具/Git/GIT的使用01-基本功能/">GIT的使用01-基本功能</a></li>  <li class="file"><a href="/hexoblog/2018/07/29/技术/工具/Git/Git查询手册/">Git 手册</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Hexo
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2016/06/12/技术/工具/Hexo/Hexo的使用02-同步/">Hexo的使用02-同步</a></li>  <li class="file"><a href="/hexoblog/2016/06/13/技术/工具/Hexo/Hexo的使用01-搭建/">Hexo的使用01-搭建</a></li>  <li class="file"><a href="/hexoblog/2017/04/12/技术/工具/Hexo/Hexo的使用05-Atom编辑器/">Hexo的使用05-Atom编辑器</a></li>  <li class="file"><a href="/hexoblog/2017/04/12/技术/工具/Hexo/Hexo的使用04-数学公式/">Hexo的使用04-数学公式</a></li>  <li class="file"><a href="/hexoblog/2017/05/05/技术/工具/Hexo/Hexo的使用03-迁移/">Hexo的使用03-迁移</a></li>  <li class="file"><a href="/hexoblog/2017/07/17/技术/工具/Hexo/Hexo的使用06-使用gist存储代码片段/">Hexo中使用gist存储代码片段</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Visio
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/05/05/技术/工具/Visio/VISIO的使用01-基础入门/">VISIO的使用01-基础入门</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            图片处理
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/05/05/技术/工具/图片处理/图片处理02-一寸照片/">图片处理02-一寸照片</a></li>  <li class="file"><a href="/hexoblog/2017/05/17/技术/工具/图片处理/图片处理01-合并多张图片/">图片处理01-合并多张图片</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            正则表达式
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2018/07/29/技术/工具/正则表达式/正则表达式/">正则表达式</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            编程IDE
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Anaconda
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/04/18/技术/工具/编程IDE/Anaconda/Anaconda的使用01-基础/">Anaconda的使用01-基础</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            JetBrainsCLion
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/05/04/技术/工具/编程IDE/JetBrainsCLion/JetBrainsCLion的使用01-入门/">JetBrainsCLion的使用01-入门</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            VSCode
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/07/24/技术/工具/编程IDE/VSCode/Visual-Studio-Code使用技巧/">Visual Studio Code使用技巧</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            网络软件
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2016/06/12/技术/工具/网络软件/Windows全局代理软件Proxifier/">Windows全局代理软件Proxifier</a></li>  <li class="file"><a href="/hexoblog/2016/06/12/技术/工具/网络软件/Windows手工修改路由表/">Windows手工修改路由表</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            数据库
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2016/06/11/技术/数据库/数据库_Redis入门/">Redis入门</a></li>  <li class="file"><a href="/hexoblog/2016/06/12/技术/数据库/数据库_使用Database-Configuration-Assist-工具创建oracle数据库/">使用Database Configuration Assist 工具创建oracle数据库</a></li>  <li class="file"><a href="/hexoblog/2017/11/02/技术/数据库/SQL/">SQL</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            数据结构和算法
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/03/21/技术/数据结构和算法/算法专题_二维数组/">算法专题_二维数组</a></li>  <li class="file"><a href="/hexoblog/2017/03/21/技术/数据结构和算法/算法专题_矩阵和图/">矩阵和用矩阵表示的图的相关问题</a></li>  <li class="file"><a href="/hexoblog/2017/03/21/技术/数据结构和算法/算法专题_位运算/">算法专题_位运算</a></li>  <li class="file"><a href="/hexoblog/2017/03/23/技术/数据结构和算法/算法专题_链表/">算法专题_链表</a></li>  <li class="file"><a href="/hexoblog/2017/04/05/技术/数据结构和算法/算法专题_动态规划/">算法专题_动态规划</a></li>  <li class="file"><a href="/hexoblog/2017/04/06/技术/数据结构和算法/算法专题_图/">算法专题_图问题</a></li>  <li class="file"><a href="/hexoblog/2017/04/08/技术/数据结构和算法/做过的算法题汇总表/">做过的算法题汇总表</a></li>  <li class="file"><a href="/hexoblog/2017/04/09/技术/数据结构和算法/算法专题_二叉树/">算法专题_二叉树</a></li>  <li class="file"><a href="/hexoblog/2017/04/09/技术/数据结构和算法/算法专题_通用树结构/">算法专题_通用树结构</a></li>  <li class="file"><a href="/hexoblog/2017/04/10/技术/数据结构和算法/算法专题_大数据和空间限制/">算法专题_大数据和空间限制</a></li>  <li class="file"><a href="/hexoblog/2017/04/10/技术/数据结构和算法/算法专题_排列组合/">算法专题_排列组合</a></li>  <li class="file"><a href="/hexoblog/2017/04/11/技术/数据结构和算法/本地代码集锦/">本地代码集锦</a></li>  <li class="file"><a href="/hexoblog/2017/04/12/技术/数据结构和算法/算法专题_栈和队列/">算法专题_栈和队列</a></li>  <li class="file"><a href="/hexoblog/2017/04/16/技术/数据结构和算法/算法专题_排序算法/">算法专题_排序算法</a></li>  <li class="file"><a href="/hexoblog/2017/04/17/技术/数据结构和算法/算法专题_算法总结/">算法专题_算法总结</a></li>  <li class="file"><a href="/hexoblog/2017/05/02/技术/数据结构和算法/算法专题_一维数组/">算法专题_一维数组</a></li>  <li class="file"><a href="/hexoblog/2017/05/05/技术/数据结构和算法/算法专题_图之网络流/">算法专题_图之网络流</a></li>  <li class="file"><a href="/hexoblog/2017/05/06/技术/数据结构和算法/算法专题_图之最短路径/">算法专题_图之最短路径</a></li>  <li class="file"><a href="/hexoblog/2017/05/06/技术/数据结构和算法/算法专题_最小生成树/">算法专题_最小生成树</a></li>  <li class="file"><a href="/hexoblog/2017/05/06/技术/数据结构和算法/算法专题_霍夫曼编码/">算法专题_霍夫曼编码(哈夫曼编码)</a></li>  <li class="file"><a href="/hexoblog/2017/05/07/技术/数据结构和算法/Kickstart-Round-B-2017/">Kickstart Round B 2017</a></li>  <li class="file"><a href="/hexoblog/2017/05/08/技术/数据结构和算法/算法专题_模运算/">算法专题_模运算</a></li>  <li class="file"><a href="/hexoblog/2017/05/09/技术/数据结构和算法/算法专题_素数问题/">算法专题_素数问题</a></li>  <li class="file"><a href="/hexoblog/2017/05/11/技术/数据结构和算法/算法专题_计算几何/">算法专题_计算几何</a></li>  <li class="file"><a href="/hexoblog/2017/05/27/技术/数据结构和算法/计蒜之道2017程序设计大赛/">计蒜之道2017程序设计大赛</a></li>  <li class="file"><a href="/hexoblog/2017/05/28/技术/数据结构和算法/算法专题_字符串匹配/">算法专题_字符串匹配</a></li>  <li class="file"><a href="/hexoblog/2017/07/12/技术/数据结构和算法/四等分数组/">四等分数组</a></li>  <li class="file"><a href="/hexoblog/2017/07/12/技术/数据结构和算法/矩阵的遍历/">矩阵的遍历</a></li>  <li class="file"><a href="/hexoblog/2017/07/20/技术/数据结构和算法/N皇后问题/">N皇后问题</a></li>  <li class="file"><a href="/hexoblog/2017/07/23/技术/数据结构和算法/算法专题_线段树/">算法专题_线段树</a></li>  <li class="file"><a href="/hexoblog/2017/07/23/技术/数据结构和算法/算法专题_树状数组/">算法专题_树状数组</a></li>  <li class="file"><a href="/hexoblog/2017/07/23/技术/数据结构和算法/算法专题_并查集/">算法专题_并查集</a></li>  <li class="file"><a href="/hexoblog/2017/07/26/技术/数据结构和算法/算法专题_链表2/">算法专题_链表2</a></li>  <li class="file"><a href="/hexoblog/2017/08/06/技术/数据结构和算法/算法专题_二叉堆/">算法专题_二叉堆</a></li>  <li class="file"><a href="/hexoblog/2017/08/07/技术/数据结构和算法/算法专题_快速排序/">算法专题_快速排序</a></li>  <li class="file"><a href="/hexoblog/2017/08/07/技术/数据结构和算法/算法专题_归并排序/">算法专题_归并排序</a></li>  <li class="file"><a href="/hexoblog/2017/08/13/技术/数据结构和算法/算法专题-hihocoder/">算法专题_hihocoder</a></li>  <li class="file"><a href="/hexoblog/2017/08/17/技术/数据结构和算法/算法专题-贪心法/">算法专题_贪心法</a></li>  <li class="file"><a href="/hexoblog/2017/08/18/技术/数据结构和算法/阿里在线测评-兔子繁殖问题/">阿里笔试</a></li>  <li class="file"><a href="/hexoblog/2017/08/22/技术/数据结构和算法/今日头条-在线编程题/">今日头条_在线编程题</a></li>  <li class="file"><a href="/hexoblog/2017/08/23/技术/数据结构和算法/算法专题-字典树-Trie树/">算法专题_字典树(Trie树)</a></li>  <li class="file"><a href="/hexoblog/2017/09/23/技术/数据结构和算法/算法专题_二叉树2/">算法专题_二叉树2</a></li>  <li class="file"><a href="/hexoblog/2017/09/27/技术/数据结构和算法/手写代码-其他/">手写代码-其他</a></li>  <li class="file"><a href="/hexoblog/2017/10/03/技术/数据结构和算法/算法专题-常见题目/">算法专题_常见题目</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            机器学习
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            深度学习
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/03/22/技术/机器学习/深度学习/深度学习_Theano使用技巧/">深度学习_Theano使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/04/18/技术/机器学习/深度学习/深度学习_基本概念/">深度学习_基本概念</a></li>  <li class="file"><a href="/hexoblog/2017/04/18/技术/机器学习/深度学习/深度学习_利用神经网络识别手写数字/">深度学习__利用神经网络识别手写数字</a></li>  <li class="file"><a href="/hexoblog/2017/04/19/技术/机器学习/深度学习/深度学习_反向传播算法及简单实例/">深度学习_反向传播算法及简单实例</a></li>  <li class="file"><a href="/hexoblog/2017/04/19/技术/机器学习/深度学习/深度学习_Keras使用技巧/">深度学习_Keras使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/04/20/技术/机器学习/深度学习/深度学习_使用keras实现autoencoder/">深度学习_使用keras实现autoencoder</a></li>  <li class="file"><a href="/hexoblog/2017/04/20/技术/机器学习/深度学习/深度学习_卷积神经网络/">深度学习_卷积神经网络</a></li>  <li class="file"><a href="/hexoblog/2017/04/27/技术/机器学习/深度学习/深度学习_使用autoencoder自动提取特征/">深度学习_使用autoencoder自动提取特征</a></li>  <li class="file"><a href="/hexoblog/2017/05/10/技术/机器学习/深度学习/深度学习_递归神经网络(RNN)/">深度学习_递归神经网络(RNN)</a></li>  <li class="file"><a href="/hexoblog/2017/05/10/技术/机器学习/深度学习/深度学习_限制波尔茨曼向量机(RBM)/">深度学习_限制波尔茨曼向量机(RBM)</a></li>  <li class="file"><a href="/hexoblog/2017/05/12/技术/机器学习/深度学习/深度学习_TensorFlow使用技巧/">深度学习_TensorFlow使用技巧</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/hexoblog/2016/06/16/技术/机器学习/机器学习_Scikit-Learn-ManyClassifier/">同时使用多个分类器(Scikit-Learn)</a></li>  <li class="file"><a href="/hexoblog/2016/06/16/技术/机器学习/机器学习_范数/">机器学习_范数</a></li>  <li class="file"><a href="/hexoblog/2016/06/20/技术/机器学习/机器学习_学习路线/">机器学习_学习路线</a></li>  <li class="file"><a href="/hexoblog/2016/06/20/技术/机器学习/机器学习_手写数字识别/">机器学习_手写数字识别</a></li>  <li class="file"><a href="/hexoblog/2016/07/11/技术/机器学习/机器学习_Matplolib使用技巧/">机器学习_Matplolib使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/03/22/技术/机器学习/机器学习_人脸识别/">机器学习_人脸识别</a></li>  <li class="file"><a href="/hexoblog/2017/05/04/技术/机器学习/机器学习_Matlab使用技巧/">机器学习_Matlab使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/05/11/技术/机器学习/机器学习_时间序列预测分析算法/">机器学习_时间序列预测分析算法</a></li>  <li class="file active"><a href="/hexoblog/2017/05/14/技术/机器学习/机器学习_Scikit-Learn使用技巧/">深度学习_Scikit-Learn机器学习算法的使用</a></li>  <li class="file"><a href="/hexoblog/2017/05/15/技术/机器学习/机器学习_时间序列预测の广告效果预测/">机器学习_时间序列预测の广告效果预测</a></li>  <li class="file"><a href="/hexoblog/2017/05/15/技术/机器学习/机器学习_算法汇总/">机器学习_算法汇总</a></li>  <li class="file"><a href="/hexoblog/2017/05/17/技术/机器学习/机器学习_Pandas使用技巧/">深度学习_Pandas使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/05/19/技术/机器学习/机器学习_感知机/">机器学习_感知机</a></li>  <li class="file"><a href="/hexoblog/2017/05/19/技术/机器学习/机器学习_逻辑回归/">机器学习_逻辑回归</a></li>  <li class="file"><a href="/hexoblog/2017/05/22/技术/机器学习/机器学习_损失函数/">机器学习_损失函数</a></li>  <li class="file"><a href="/hexoblog/2017/06/03/技术/机器学习/机器学习_分类器性能的度量/">机器学习_分类器性能的度量</a></li>  <li class="file"><a href="/hexoblog/2017/06/15/技术/机器学习/机器学习_Scipy使用技巧/">机器学习_Scipy使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/06/15/技术/机器学习/机器学习_Python使用技巧/">深度学习_Python使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/07/23/技术/机器学习/机器学习-Numpy使用技巧/">机器学习_Numpy使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/10/15/技术/机器学习/KNN-with-C/">KNN with C++</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-绪论-基本概念/">机器学习-绪论-基本概念</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-第一章-逻辑回归/">机器学习-第一章-逻辑回归</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-第二章-决策树/">机器学习-第二章-决策树</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-第三章-朴素贝叶斯/">机器学习-第三章-朴素贝叶斯</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-第四章-支持向量机/">机器学习-第四章-支持向量机</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-第五章-最近邻/">机器学习-第五章-最近邻</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-第六章-kmeans/">机器学习-第六章-kmeans</a></li>  <li class="file"><a href="/hexoblog/2017/11/04/技术/机器学习/机器学习-第七章-感知机/">机器学习-第七章-感知机</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            编程语言
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            C++
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/04/07/技术/编程语言/C++/C++语言技巧/">C++语言技巧</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            计算机基础
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            计算机网络
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2016/06/12/技术/计算机基础/计算机网络/网络_校园网多终端上网方案/">校园网多终端上网方案</a></li>  <li class="file"><a href="/hexoblog/2017/07/12/技术/计算机基础/计算机网络/计算机网络/">计算机网络</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            认知神经科学
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2016/06/12/技术/认知神经科学/文献检索方法/">文献检索方法</a></li>  <li class="file"><a href="/hexoblog/2016/06/12/技术/认知神经科学/参考文献书写格式/">参考文献书写格式</a></li>  <li class="file"><a href="/hexoblog/2016/06/12/技术/认知神经科学/使用Python处理fMRI数据/">使用Python处理fMRI数据</a></li>  <li class="file"><a href="/hexoblog/2016/06/22/技术/认知神经科学/中英文对照/">中英文对照</a></li>  <li class="file"><a href="/hexoblog/2017/01/03/技术/认知神经科学/circos入门教程/">circos入门教程</a></li>  <li class="file"><a href="/hexoblog/2017/03/08/技术/认知神经科学/基于视频的车牌识别和流量统计/">基于视频的车牌识别和流量统计</a></li>  <li class="file"><a href="/hexoblog/2017/03/08/技术/认知神经科学/基于贝叶斯网络和隐性知识的AU识别研究/">基于贝叶斯网络和隐性知识的AU识别研究</a></li>  <li class="file"><a href="/hexoblog/2017/03/09/技术/认知神经科学/基于连接的脑信息解码研究/">基于连接信息的脑信息解码研究</a></li>  <li class="file"><a href="/hexoblog/2017/03/09/技术/认知神经科学/认知神经科学系列目录/">认知神经科学系列目录</a></li>  <li class="file"><a href="/hexoblog/2017/03/20/技术/认知神经科学/多被试多RUN批量预处理(SPM)/">多被试多RUN批量预处理(SPM)</a></li>  <li class="file"><a href="/hexoblog/2017/03/20/技术/认知神经科学/SPM预处理中的常用操作/">SPM预处理中的常用操作</a></li>  <li class="file"><a href="/hexoblog/2017/03/20/技术/认知神经科学/动态因果模型(DCM)的批量定义和估计/">动态因果模型(DCM)的批量定义和估计</a></li>  <li class="file"><a href="/hexoblog/2017/04/13/技术/认知神经科学/使用SPM做Second-Level分析/">使用SPM做Second_Level分析</a></li>  <li class="file"><a href="/hexoblog/2017/04/13/技术/认知神经科学/DCM模型的定义和估计/">DCM模型的定义和估计</a></li>  <li class="file"><a href="/hexoblog/2017/04/14/技术/认知神经科学/XJVIEW的使用技巧/">XJVIEW的使用技巧</a></li>  <li class="file"><a href="/hexoblog/2017/04/14/技术/认知神经科学/SPM中函数的修改和使用/">SPM中函数的修改和使用</a></li>  <li class="file"><a href="/hexoblog/2017/05/02/技术/认知神经科学/fMRI中常用的工具包/">fMRI中常用的工具包</a></li>  <li class="file"><a href="/hexoblog/2017/05/11/技术/认知神经科学/使用3D卷积神经神经网络提取脑成像数据的特征/">使用3D卷积神经神经网络提取脑成像数据的特征</a></li>  <li class="file"><a href="/hexoblog/2017/05/14/技术/认知神经科学/fMRI相关的资源汇总/">fMRI相关的资源汇总</a></li>  <li class="file"><a href="/hexoblog/2017/05/30/技术/认知神经科学/fMRI相关问题汇总/">fMRI相关问题汇总</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            收藏夹
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/hexoblog/2017/03/22/收藏夹/博客集锦/">博客收藏</a></li>  </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-技术/机器学习/机器学习_Scikit-Learn使用技巧" class="article article-type-post" itemscope="" itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/hexoblog/categories/技术/">技术</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/hexoblog/categories/技术/机器学习/">机器学习</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/hexoblog/tags/python/">python</a>, <a class="tag-link" href="/hexoblog/tags/scikit-learn/">scikit-learn</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/hexoblog/2017/05/14/技术/机器学习/机器学习_Scikit-Learn使用技巧/">
            <time datetime="2017-05-14T10:51:20.000Z" itemprop="datePublished">2017-05-14</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href="https://github.com/FF120/hexoblog/raw/master/source/_posts/技术/机器学习/机器学习_Scikit-Learn使用技巧.md"> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href="https://github.com/FF120/hexoblog/edit/master/source/_posts/技术/机器学习/机器学习_Scikit-Learn使用技巧.md"> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href="https://github.com/FF120/hexoblog/commits/master/source/_posts/技术/机器学习/机器学习_Scikit-Learn使用技巧.md"> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            深度学习_Scikit-Learn机器学习算法的使用
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">Catalogue</strong>
                    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#预测结果与真实结果的比较"><span class="toc-number">1.</span> <span class="toc-text">预测结果与真实结果的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#划分数据集"><span class="toc-number">2.</span> <span class="toc-text">划分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征选择方法"><span class="toc-number">3.</span> <span class="toc-text">特征选择方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#univariate-feature-selection-单变量特征选择"><span class="toc-number">3.1.</span> <span class="toc-text">Univariate feature selection （单变量特征选择）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#主要的选择策略"><span class="toc-number">3.2.</span> <span class="toc-text">主要的选择策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#具体应用"><span class="toc-number">3.3.</span> <span class="toc-text">具体应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#recursive-feature-elimination-递归特征消除"><span class="toc-number">3.4.</span> <span class="toc-text">Recursive feature elimination （递归特征消除）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#feature-selection-using-selectfrommodel从模型中选择特征"><span class="toc-number">3.5.</span> <span class="toc-text">Feature selection using SelectFromModel(从模型中选择特征)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类器"><span class="toc-number">4.</span> <span class="toc-text">分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#支持向量机svm"><span class="toc-number">4.1.</span> <span class="toc-text">支持向量机（SVM）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#简介"><span class="toc-number">4.1.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#用途"><span class="toc-number">4.1.2.</span> <span class="toc-text">用途</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#优点"><span class="toc-number">4.1.3.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#缺点"><span class="toc-number">4.1.4.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#使用方法"><span class="toc-number">4.1.5.</span> <span class="toc-text">使用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#二分类"><span class="toc-number">4.1.5.1.</span> <span class="toc-text">二分类</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#多分类"><span class="toc-number">4.1.5.2.</span> <span class="toc-text">多分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#模型参数说明"><span class="toc-number">4.2.</span> <span class="toc-text">模型参数说明</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#linearsvc"><span class="toc-number">4.2.1.</span> <span class="toc-text">LinearSVC</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#svc"><span class="toc-number">4.2.2.</span> <span class="toc-text">SVC</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#nusvc"><span class="toc-number">4.2.3.</span> <span class="toc-text">NuSVC</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#查看训练好的模型的参数"><span class="toc-number">4.3.</span> <span class="toc-text">查看训练好的模型的参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#决策函数decision-function"><span class="toc-number">4.4.</span> <span class="toc-text">决策函数（decision function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#核函数kernel-function"><span class="toc-number">4.5.</span> <span class="toc-text">核函数（kernel function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#随机梯度下降stochastic-gradient-descen"><span class="toc-number">4.6.</span> <span class="toc-text">随机梯度下降（Stochastic Gradient Descen）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#用途-1"><span class="toc-number">4.6.1.</span> <span class="toc-text">用途</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#优点-1"><span class="toc-number">4.6.2.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#缺点-1"><span class="toc-number">4.6.3.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#使用方法-1"><span class="toc-number">4.6.4.</span> <span class="toc-text">使用方法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#最近邻方法nearest-neighbors"><span class="toc-number">4.7.</span> <span class="toc-text">最近邻方法（Nearest Neighbors）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#简介-1"><span class="toc-number">4.7.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#用法"><span class="toc-number">4.7.2.</span> <span class="toc-text">用法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型持久化"><span class="toc-number">5.</span> <span class="toc-text">模型持久化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#结果的可视化"><span class="toc-number">6.</span> <span class="toc-text">结果的可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据预处理"><span class="toc-number">7.</span> <span class="toc-text">数据预处理</span></a></li></ol>
                </div>
            
        
        
            <p><code>scikit-learn</code>是一个很受欢迎的机器学习方面的<code>python</code>工具包，它定义的一些范式和处理流程影响深远，所以，认识和了解一些这个工具包对于自己实现一些机器学习算法是很有帮助的。它已经实现了很多方法帮助我们便捷的处理数据，例如，划分数据集为训练集和验证集，交叉验证，数据预处理，归一化等等。</p>
<a id="more"></a>
<h3 id="预测结果与真实结果的比较">预测结果与真实结果的比较</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算均方误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">rmse = sqrt(metrics.mean_squared_error(y_test, y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">acc = metrics.accuracy_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line">cm = metrics.confusion_matrix(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># classification_report</span></span><br><span class="line">cr = metrics.classification_report(y_true, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ROC AUC曲线</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc</span><br></pre></td></tr></table></figure>
<h3 id="划分数据集">划分数据集</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cross_validation</span><br><span class="line">X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分折</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> KFold</span><br><span class="line">kf = KFold(n_samples, n_folds=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保证不同的类别之间的均衡，这里需要用到标签labels</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">labels = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">skf = StratifiedKFold(labels, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> skf:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留一交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> LeaveOneOut</span><br><span class="line">loo = LeaveOneOut(n_samples)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> loo:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留P交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> LeavePOut</span><br><span class="line">lpo = LeavePOut(n_samples, p=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> lpo:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照额外提供的标签留一交叉验证,常用的情况是按照时间序列</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> LeaveOneLabelOut</span><br><span class="line">labels = [<span class="number">1</span>, <span class="number">1</span>,<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">lolo = LeaveOneLabelOut(labels)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> lolo:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照额外提供的标签留P交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> LeavePLabelOut</span><br><span class="line">labels = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">lplo = LeavePLabelOut(labels, p=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> lplo:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机分组</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> ShuffleSplit</span><br><span class="line">ss = ShuffleSplit(<span class="number">16</span>, n_iter=<span class="number">3</span>, test_size=<span class="number">0.25</span>,random_state=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> ss:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train_index, test_index))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 考虑类别均衡的随机分组</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">sss = StratifiedShuffleSplit(y, <span class="number">3</span>, test_size=<span class="number">0.5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> sss:</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br></pre></td></tr></table></figure>
<h3 id="特征选择方法">特征选择方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除方差较小的特征</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> feature_selection</span><br><span class="line">vt = feature_selection.VarianceThreshold(threshold=<span class="string">''</span>)</span><br><span class="line">vt.fit(X_train)</span><br><span class="line">X_train_transformed = vt.transform(X_train)</span><br><span class="line">X_test_transformed = vt.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照某种排序规则 选择前K个特征</span></span><br><span class="line"><span class="comment"># 除了使用系统定义好的函数f_classif，还可以自己定义函数</span></span><br><span class="line">sk = SelectKBest(feature_selection.f_classif,k=<span class="number">100</span>)</span><br><span class="line">sk.fit(X_train,y_train)</span><br><span class="line">X_train_transformed = sk.transform(X_train)</span><br><span class="line">X_test_transformed = sk.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归特征消除</span></span><br><span class="line">rfecv = RFECV(estimator=svc, step=step, cv=StratifiedKFold(y, n_folds = n_folds),scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">rfecv.fit(X_train, y_train)</span><br><span class="line">X_train_transformed = rfecv.transform(X_train)</span><br><span class="line">X_test_transformed = rfecv.transform(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用L1做特征选择</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line">lsvc = LinearSVC(C=<span class="number">1</span>, penalty=<span class="string">"l1"</span>, dual=<span class="literal">False</span>)</span><br><span class="line">lsvc.fit(X_train,y_train)</span><br><span class="line">X_train_transformed = lsvc.transform(X_train)</span><br><span class="line">X_test_transformed = lsvc.transform(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于树的特征选择</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line">etc = ExtraTreesClassifier()</span><br><span class="line">etc.fit(X_train, y_train)</span><br><span class="line">X_train_transformed = etc.transform(X_train)</span><br><span class="line">X_test_transformed = etc.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于线性判别分析做特征选择</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line">lda = LinearDiscriminantAnalysis(solver=<span class="string">'lsqr'</span>,shrinkage=<span class="string">'auto'</span>)</span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line">X_train_transformed = lda.transform(X_train)</span><br><span class="line">X_test_transformed = lda.transform(X_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">sel = VarianceThreshold(threshold=(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;sel</span><br><span class="line">&gt;&gt;&gt;VarianceThreshold(threshold=<span class="number">0.16</span>)</span><br><span class="line"></span><br><span class="line">X2 = sel.fit_transform(X)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;X2</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<p>计算每维特征的方差 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a1 = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;a1.var()</span><br><span class="line">&gt;&gt;&gt;<span class="number">0.13888888888888892</span></span><br><span class="line"></span><br><span class="line">a2 = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;a2.var()</span><br><span class="line">&gt;&gt;&gt;Out[<span class="number">161</span>]: <span class="number">0.22222222222222224</span></span><br><span class="line"></span><br><span class="line">a3 = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;a3.var()</span><br><span class="line">&gt;&gt;&gt;Out[<span class="number">163</span>]: <span class="number">0.25</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，方差小于0.16的只有第一维特征，所以X2保留下来的是原来的第二维和第三维特征。 &gt;这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。</p>
<h4 id="univariate-feature-selection-单变量特征选择">Univariate feature selection （单变量特征选择）</h4>
<p>主要使用统计的方法计算各个统计值，再根据一定的阈值筛选出符合要求的特征，去掉不符合要求的特征。 #### 主要的统计方法 - F值分类 <code>f_classif</code> - F值回归 <code>f_regression</code> - 卡方统计 <code>chi2</code> (适用于非负特征值 和 稀疏特征值)</p>
<h4 id="主要的选择策略">主要的选择策略</h4>
<ul>
<li>选择排名前K的特征 <code>SelectKbest</code></li>
<li>选择前百分之几的特征 <code>SelectPercentile</code></li>
<li><code>SelectFpr</code> Select features based on a false positive rate test.</li>
<li><code>SelectFdr</code> Select features based on an estimated false discovery rate.</li>
<li><code>SelectFwe</code> Select features based on family-wise error rate.</li>
<li><code>GenericUnivariateSelect</code> Univariate feature selector with configurable mode.</li>
</ul>
<blockquote>
<p><code>false positive rate</code>: FP / (FP + TP) 假设类别为0，1；记0为negative,1为positive, <code>FPR</code>就是实际的类别是0，但是分类器错误的预测为1的个数 与 分类器预测的类别为1的样本的总数（包括正确的预测为1和错误的预测为1） 的比值。 <code>estimated false discovery rate</code>: 错误的拒绝原假设的概率 <code>family-wise error rate</code>: 至少有一个检验犯第一类错误的概率</p>
</blockquote>
<p>假设检验的两类错误： &gt; - 第一类错误：原假设是正确的，但是却被拒绝了。(用α表示） &gt; - 第二类错误：原假设是错误的，但是却被接受了。(用β表示)</p>
<h4 id="具体应用">具体应用</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="comment">#SelectKBest -- f_classif</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">X_fitted = SelectKBest(f_classif, k=<span class="number">3</span>).fit(X,y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"SelectKBest -- f_classif"</span></span><br><span class="line"><span class="keyword">print</span> X_fitted.scores_</span><br><span class="line"><span class="keyword">print</span> X_fitted.pvalues_</span><br><span class="line"><span class="keyword">print</span> X_fitted.get_support()</span><br><span class="line">X_transformed = X_fitted.transform(X)</span><br><span class="line"><span class="keyword">print</span> X_transformed.shape</span><br><span class="line"><span class="comment">#SelectKBest -- chi2</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">X_fitted_2 = SelectKBest(chi2, k=<span class="number">3</span>).fit(X,y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"SelectKBest -- chi2"</span></span><br><span class="line"><span class="keyword">print</span> X_fitted_2.scores_</span><br><span class="line"><span class="keyword">print</span> X_fitted_2.pvalues_</span><br><span class="line"><span class="keyword">print</span> X_fitted_2.get_support()</span><br><span class="line">X_transformed_2 = X_fitted_2.transform(X)</span><br><span class="line"><span class="keyword">print</span> X_transformed_2.shape</span><br><span class="line"></span><br><span class="line"><span class="comment">#SelectPercentile -- f_classif</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line">X_fitted_3 = SelectPercentile(f_classif, percentile=<span class="number">50</span>).fit(X,y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"SelectPercentile -- f_classif"</span></span><br><span class="line"><span class="keyword">print</span> X_fitted_3.scores_</span><br><span class="line"><span class="keyword">print</span> X_fitted_3.pvalues_</span><br><span class="line"><span class="keyword">print</span> X_fitted_3.get_support()</span><br><span class="line">X_transformed_3 = X_fitted_3.transform(X)</span><br><span class="line"><span class="keyword">print</span> X_transformed_3.shape</span><br><span class="line"></span><br><span class="line"><span class="comment">#SelectPercentile -- chi2</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">X_fitted_4 = SelectPercentile(chi2, percentile=<span class="number">50</span>).fit(X,y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"SelectPercentile -- chi2"</span></span><br><span class="line"><span class="keyword">print</span> X_fitted_4.scores_</span><br><span class="line"><span class="keyword">print</span> X_fitted_4.pvalues_</span><br><span class="line"><span class="keyword">print</span> X_fitted_4.get_support()</span><br><span class="line">X_transformed_4 = X_fitted_4.transform(X)</span><br><span class="line"><span class="keyword">print</span> X_transformed_4.shape</span><br><span class="line"></span><br><span class="line"><span class="comment">#SelectFpr --- chi2</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFpr</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">X_fitted_5 = SelectFpr(chi2, alpha=<span class="number">2.50017968e-15</span>).fit(X,y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"SelectFpr --- chi2"</span></span><br><span class="line"><span class="keyword">print</span> X_fitted_5.scores_</span><br><span class="line"><span class="keyword">print</span> X_fitted_5.pvalues_</span><br><span class="line"><span class="keyword">print</span> X_fitted_5.get_support()</span><br><span class="line">X_transformed_5 = X_fitted_5.transform(X)</span><br><span class="line"><span class="keyword">print</span> X_transformed_5.shape</span><br><span class="line"></span><br><span class="line"><span class="comment">#SelectFpr --- f_classif</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFpr</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line">X_fitted_6 = SelectFpr(f_classif, alpha=<span class="number">1.66966919e-31</span> ).fit(X,y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"SelectFpr --- f_classif"</span></span><br><span class="line"><span class="keyword">print</span> X_fitted_6.scores_</span><br><span class="line"><span class="keyword">print</span> X_fitted_6.pvalues_</span><br><span class="line"><span class="keyword">print</span> X_fitted_6.get_support()</span><br><span class="line">X_transformed_6 = X_fitted_6.transform(X)</span><br><span class="line"><span class="keyword">print</span> X_transformed_6.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># SelectFdr  和 SelectFwe 的用法和上面类似，只是选择特征时候的依据不同，真正决定得分不同的是</span></span><br><span class="line"><span class="comment">#统计检验方法，从上面可以看到，使用f_classif的得出的参数都相同。</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">SelectKBest -- f_classif</span><br><span class="line">[  <span class="number">119.26450218</span>    <span class="number">47.3644614</span>   <span class="number">1179.0343277</span>    <span class="number">959.32440573</span>]</span><br><span class="line">[  <span class="number">1.66966919e-31</span>   <span class="number">1.32791652e-16</span>   <span class="number">3.05197580e-91</span>   <span class="number">4.37695696e-85</span>]</span><br><span class="line">[ <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>]</span><br><span class="line">(<span class="number">150L</span>, <span class="number">3L</span>)</span><br><span class="line">SelectKBest -- chi2</span><br><span class="line">[  <span class="number">10.81782088</span>    <span class="number">3.59449902</span>  <span class="number">116.16984746</span>   <span class="number">67.24482759</span>]</span><br><span class="line">[  <span class="number">4.47651499e-03</span>   <span class="number">1.65754167e-01</span>   <span class="number">5.94344354e-26</span>   <span class="number">2.50017968e-15</span>]</span><br><span class="line">[ <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>]</span><br><span class="line">(<span class="number">150L</span>, <span class="number">3L</span>)</span><br><span class="line">SelectPercentile -- f_classif</span><br><span class="line">[  <span class="number">119.26450218</span>    <span class="number">47.3644614</span>   <span class="number">1179.0343277</span>    <span class="number">959.32440573</span>]</span><br><span class="line">[  <span class="number">1.66966919e-31</span>   <span class="number">1.32791652e-16</span>   <span class="number">3.05197580e-91</span>   <span class="number">4.37695696e-85</span>]</span><br><span class="line">[<span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>]</span><br><span class="line">(<span class="number">150L</span>, <span class="number">2L</span>)</span><br><span class="line">SelectPercentile -- chi2</span><br><span class="line">[  <span class="number">10.81782088</span>    <span class="number">3.59449902</span>  <span class="number">116.16984746</span>   <span class="number">67.24482759</span>]</span><br><span class="line">[  <span class="number">4.47651499e-03</span>   <span class="number">1.65754167e-01</span>   <span class="number">5.94344354e-26</span>   <span class="number">2.50017968e-15</span>]</span><br><span class="line">[<span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>]</span><br><span class="line">(<span class="number">150L</span>, <span class="number">2L</span>)</span><br><span class="line">SelectFpr --- chi2</span><br><span class="line">[  <span class="number">10.81782088</span>    <span class="number">3.59449902</span>  <span class="number">116.16984746</span>   <span class="number">67.24482759</span>]</span><br><span class="line">[  <span class="number">4.47651499e-03</span>   <span class="number">1.65754167e-01</span>   <span class="number">5.94344354e-26</span>   <span class="number">2.50017968e-15</span>]</span><br><span class="line">[<span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span>]</span><br><span class="line">(<span class="number">150L</span>, <span class="number">1L</span>)</span><br><span class="line">SelectFpr --- f_classif</span><br><span class="line">[  <span class="number">119.26450218</span>    <span class="number">47.3644614</span>   <span class="number">1179.0343277</span>    <span class="number">959.32440573</span>]</span><br><span class="line">[  <span class="number">1.66966919e-31</span>   <span class="number">1.32791652e-16</span>   <span class="number">3.05197580e-91</span>   <span class="number">4.37695696e-85</span>]</span><br><span class="line">[<span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>]</span><br><span class="line">(<span class="number">150L</span>, <span class="number">2L</span>)</span><br></pre></td></tr></table></figure>
<h4 id="recursive-feature-elimination-递归特征消除">Recursive feature elimination （递归特征消除）</h4>
<p>使用某种方法，给每一维特征赋一个权重（例如线性回归的系数），去除系数最小的K个特征，然后在剩下的特征上重复上述方法，直到剩下的特征满足特征选择个数的要求。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">用SVM获得每个特征对分类结果的贡献程度，按照贡献程度从大到小排名，选出贡献程度最大的</span></span><br><span class="line"><span class="string">前K个特征作为特征选择的结果,使用SVM的时候，排名的依据是fit之后的coef_值。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这里的估计器可以替换成任何其他方法，如GLM</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Load the digits dataset</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.images.reshape((len(digits.images), <span class="number">-1</span>))</span><br><span class="line">y = digits.target</span><br><span class="line"><span class="keyword">print</span> <span class="string">"原来的特征："</span></span><br><span class="line"><span class="keyword">print</span> X.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the RFE object and rank each pixel</span></span><br><span class="line">svc = SVC(kernel=<span class="string">"linear"</span>, C=<span class="number">1</span>)</span><br><span class="line">rfe = RFE(estimator=svc, n_features_to_select=<span class="number">10</span>, step=<span class="number">1</span>)</span><br><span class="line">ref = rfe.fit(X, y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"选择的特征的个数"</span></span><br><span class="line"><span class="keyword">print</span> np.sum(ref._get_support_mask())</span><br><span class="line"><span class="keyword">print</span> ref._get_support_mask()</span><br><span class="line"><span class="keyword">print</span> rfe.ranking_</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">原来的特征：</span><br><span class="line">(<span class="number">1797L</span>, <span class="number">64L</span>)</span><br><span class="line">选择的特征的个数</span><br><span class="line"><span class="number">10</span></span><br><span class="line">[<span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span></span><br><span class="line"> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span></span><br><span class="line"> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span></span><br><span class="line"> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span></span><br><span class="line"> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span></span><br><span class="line"> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>]</span><br><span class="line">[<span class="number">55</span> <span class="number">41</span> <span class="number">22</span> <span class="number">14</span>  <span class="number">1</span>  <span class="number">8</span> <span class="number">25</span> <span class="number">42</span> <span class="number">48</span> <span class="number">28</span> <span class="number">21</span> <span class="number">34</span>  <span class="number">5</span> <span class="number">23</span> <span class="number">35</span> <span class="number">43</span> <span class="number">45</span> <span class="number">32</span> <span class="number">10</span>  <span class="number">6</span> <span class="number">19</span>  <span class="number">1</span> <span class="number">30</span> <span class="number">44</span> <span class="number">46</span></span><br><span class="line"> <span class="number">36</span>  <span class="number">1</span>  <span class="number">9</span> <span class="number">11</span> <span class="number">29</span>  <span class="number">1</span> <span class="number">50</span> <span class="number">54</span> <span class="number">33</span> <span class="number">16</span> <span class="number">26</span> <span class="number">20</span>  <span class="number">7</span>  <span class="number">1</span> <span class="number">53</span> <span class="number">52</span> <span class="number">31</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">4</span>  <span class="number">1</span>  <span class="number">1</span> <span class="number">49</span> <span class="number">47</span> <span class="number">38</span></span><br><span class="line"> <span class="number">17</span> <span class="number">27</span> <span class="number">15</span>  <span class="number">1</span> <span class="number">13</span> <span class="number">39</span> <span class="number">51</span> <span class="number">40</span>  <span class="number">1</span> <span class="number">18</span> <span class="number">24</span> <span class="number">12</span>  <span class="number">3</span> <span class="number">37</span>]</span><br></pre></td></tr></table></figure></p>
<p>使用上面的方法，需要人为的确定最后输出的特征的个数，如果不知道需要多少特征才能达到好的效果，可以使用下面的交叉验证方法自动确定输出几个特征最优。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"></span><br><span class="line"><span class="comment">#产生人工数据</span></span><br><span class="line"><span class="comment"># Build a classification task using 3 informative features</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">25</span>, n_informative=<span class="number">5</span>,</span><br><span class="line">                           n_redundant=<span class="number">2</span>, n_repeated=<span class="number">0</span>, n_classes=<span class="number">8</span>,</span><br><span class="line">                           n_clusters_per_class=<span class="number">1</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the RFE object and compute a cross-validated score.</span></span><br><span class="line">svc = SVC(kernel=<span class="string">"linear"</span>)</span><br><span class="line"><span class="comment"># The "accuracy" scoring is proportional to the number of correct</span></span><br><span class="line"><span class="comment"># classifications</span></span><br><span class="line">rfecv = RFECV(estimator=svc, step=<span class="number">1</span>, cv=StratifiedKFold(y, <span class="number">5</span>),</span><br><span class="line">              scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">rfecv = rfecv.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Optimal number of features : %d"</span> % rfecv.n_features_)</span><br><span class="line">print(<span class="string">"选择的特征："</span>)</span><br><span class="line"><span class="keyword">print</span> rfecv.support_</span><br></pre></td></tr></table></figure></p>
<h4 id="feature-selection-using-selectfrommodel从模型中选择特征">Feature selection using SelectFromModel(从模型中选择特征)</h4>
<p>许多估计模型在执行完fit方法以后都会有<code>coef_</code>参数，这个参数实际上是各个特征的权重，所以我们可以根据这个权重选择特征，把权重小的特征去除。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the boston dataset.</span></span><br><span class="line">boston = load_boston()</span><br><span class="line">X, y = boston[<span class="string">'data'</span>], boston[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># We use the base estimator LassoCV since the L1 norm promotes sparsity of features.</span></span><br><span class="line">clf = LassoCV()</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"><span class="comment"># Set a minimum threshold of 0.25</span></span><br><span class="line">sfm = SelectFromModel(clf, threshold=<span class="string">'mean'</span>,prefit=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> X.shape</span><br><span class="line"><span class="comment">#sfm = sfm.fit(X, y)</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"============LassoCV================"</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"选择的特征"</span></span><br><span class="line"><span class="keyword">print</span> sfm._get_support_mask();</span><br><span class="line">n_features = sfm.transform(X).shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> n_features</span><br><span class="line"></span><br><span class="line"><span class="comment"># We use LinearSVC</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="comment">#C 越小，选择的特征越少</span></span><br><span class="line">lsvc = LinearSVC(C=<span class="number">0.001</span>, penalty=<span class="string">"l1"</span>, dual=<span class="literal">False</span>)</span><br><span class="line">y = y.astype(np.int64) <span class="comment">#转换成整数，因为是分类器，不是回归</span></span><br><span class="line">lsvc.fit(X,y)</span><br><span class="line">model = SelectFromModel(lsvc, prefit=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"============线性SVM==============================="</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"选择的特征"</span></span><br><span class="line"><span class="keyword">print</span> model._get_support_mask();</span><br><span class="line">n_features = model.transform(X).shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> n_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">clf = linear_model.LogisticRegression(C=<span class="number">0.001</span>, penalty=<span class="string">'l1'</span>, tol=<span class="number">1e-6</span>)</span><br><span class="line">y = y.astype(np.int64) <span class="comment">#转换成整数，因为是分类器，不是回归</span></span><br><span class="line">clf.fit(X,y)</span><br><span class="line">model = SelectFromModel(clf, prefit=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"============逻辑回归==============================="</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"选择的特征"</span></span><br><span class="line"><span class="keyword">print</span> model._get_support_mask();</span><br><span class="line">n_features = model.transform(X).shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> n_features</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line">clf = ExtraTreesClassifier()</span><br><span class="line">y = y.astype(np.int64) <span class="comment">#转换成整数，因为是分类器，不是回归</span></span><br><span class="line">clf = clf.fit(X, y)</span><br><span class="line">model = SelectFromModel(clf, prefit=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"============基于树的特征选择==============================="</span></span><br><span class="line"><span class="keyword">print</span> clf.feature_importances_</span><br><span class="line"><span class="keyword">print</span> <span class="string">"选择的特征："</span></span><br><span class="line"><span class="keyword">print</span> model._get_support_mask();</span><br><span class="line">n_features = model.transform(X).shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> n_features</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">(<span class="number">506L</span>, <span class="number">13L</span>)</span><br><span class="line">============LassoCV================</span><br><span class="line">选择的特征</span><br><span class="line">[<span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span></span><br><span class="line">  <span class="literal">True</span>]</span><br><span class="line"><span class="number">4</span></span><br><span class="line">============线性SVM===============================</span><br><span class="line">选择的特征</span><br><span class="line">[<span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span></span><br><span class="line"> <span class="literal">False</span>]</span><br><span class="line"><span class="number">4</span></span><br><span class="line">============逻辑回归===============================</span><br><span class="line">选择的特征</span><br><span class="line">[<span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span></span><br><span class="line"> <span class="literal">False</span>]</span><br><span class="line"><span class="number">2</span></span><br><span class="line">============基于树的特征选择===============================</span><br><span class="line">[ <span class="number">0.12196356</span>  <span class="number">0.02193675</span>  <span class="number">0.03935991</span>  <span class="number">0.01633832</span>  <span class="number">0.0721041</span>   <span class="number">0.13938681</span></span><br><span class="line">  <span class="number">0.11703915</span>  <span class="number">0.10962258</span>  <span class="number">0.03116833</span>  <span class="number">0.04455059</span>  <span class="number">0.04134067</span>  <span class="number">0.1074465</span></span><br><span class="line">  <span class="number">0.13774273</span>]</span><br><span class="line">选择的特征</span><br><span class="line">[ <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span></span><br><span class="line">  <span class="literal">True</span>]</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure></p>
<h3 id="分类器">分类器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># linear_model</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line">lmlr = linear_model.LinearRegression()</span><br><span class="line">lmlr.fit(X_train,y_train)</span><br><span class="line">lmlr.coef_</span><br><span class="line">predicted_y = lmlr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># L1 惩罚项</span></span><br><span class="line">lmr = linear_model.Ridge (alpha = <span class="number">.5</span>)</span><br><span class="line">lmr.fit(X_train,y_train)</span><br><span class="line">lmr.coef_</span><br><span class="line">lmr.intercept_</span><br><span class="line">predicted_y = lmr.predict(X_test)</span><br><span class="line"></span><br><span class="line">lmrcv = linear_model.RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">0.5</span>,<span class="number">1.0</span>, <span class="number">10.0</span>]) <span class="comment"># 自带交叉验证</span></span><br><span class="line">lmrcv.fit(X_train,y_train)</span><br><span class="line">lmrcv.alpha_</span><br><span class="line">predicted_y = lmrcv.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># L2 惩罚项</span></span><br><span class="line">lmla = linear_model.Lasso(alpha = <span class="number">0.001</span>)</span><br><span class="line">lmla.fit(X_train,y_train)</span><br><span class="line">predicted_y = lmla.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># L1 + L2 惩罚项的一个混合</span></span><br><span class="line">lmela = linear_model.ElasticNet(alpha=<span class="number">0.01</span>,l1_ratio=<span class="number">0.9</span>)</span><br><span class="line">lmela.fit(X_train,y_train)</span><br><span class="line">predicted_y = lmela.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Least Angle Regression : 适用于高维数据，缺点是对噪声比较敏感</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">lmlar = linear_model.Lars(n_nonzero_coefs=<span class="number">10</span>)</span><br><span class="line">lmlar.fit(X_train,y_train)</span><br><span class="line">predicted_y = lmlar.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">BayesianRidge : Bayesian Ridge Regression</span></span><br><span class="line"><span class="string">小特征数目表现不佳</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">lmbr = linear_model.BayesianRidge()</span><br><span class="line">lmbr.fit(X_train,y_train)</span><br><span class="line">lmbr.coef_</span><br><span class="line">predicted_y = lmbr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">ARDRegression : similar to BayesianRidge, but tend to sparse</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">lmardr = linear_model.ARDRegression(compute_score=<span class="literal">True</span>)</span><br><span class="line">lmardr.fit(X_train, y_train)</span><br><span class="line">predicted_y = lmardr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">逻辑回归</span></span><br><span class="line"><span class="string">Logistic regression</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">lmlr1 = linear_model.LogisticRegression(C=<span class="number">1</span>, penalty=<span class="string">'l1'</span>, tol=<span class="number">0.01</span>)</span><br><span class="line">lmlr2 = linear_model.LogisticRegression(C=<span class="number">1</span>, penalty=<span class="string">'l2'</span>, tol=<span class="number">0.01</span>)</span><br><span class="line">lmlr1.fit(X_train,y_train)</span><br><span class="line">predicted_y = lmlr1.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">SGDClassifier</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">lmsdg = linear_model.SGDClassifier()</span><br><span class="line">lmsdg.fit(X_train,y_train)</span><br><span class="line">predicted_y = lmsdg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Perceptron : 感知机算法</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">lmper = linear_model.Perceptron()</span><br><span class="line">lmper.fit(X_train,y_train)</span><br><span class="line">predicted_y = lmper.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">PassiveAggressiveClassifier : similar to Perceptron but have peny</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">lmpac = linear_model.PassiveAggressiveClassifier()</span><br><span class="line">lmpac.fit(X_train,y_test)</span><br><span class="line">predicted_y = lmpac.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Linear discriminant analysis  &amp;&amp; quadratic discriminant analysis</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn.lda <span class="keyword">import</span> LDA</span><br><span class="line">lda = LDA(solver=<span class="string">"svd"</span>, store_covariance=<span class="literal">True</span>)</span><br><span class="line">lda.fit(X, y)</span><br><span class="line">predicted_y = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.qda <span class="keyword">import</span> QDA</span><br><span class="line">qda = QDA()</span><br><span class="line">qda.fit(X, y, store_covariances=<span class="literal">True</span>)</span><br><span class="line">predicted_y = qda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Kernel ridge regression:</span></span><br><span class="line"><span class="string">combines Ridge Regression (linear least squares with l2-norm regularization)</span></span><br><span class="line"><span class="string">with the kernel trick</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn.kernel_ridge <span class="keyword">import</span> KernelRidge</span><br><span class="line">kr = KernelRidge(alpha=<span class="number">0.1</span>)</span><br><span class="line">kr.fit(X,y)</span><br><span class="line">predicted_y = kr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Support Vector Machines : 支持向量机分类</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">svmsvc = svm.SVC(C=<span class="number">0.1</span>,kernel=<span class="string">'rbf'</span>)</span><br><span class="line">svmsvc.fit(X_train,y_train)</span><br><span class="line">svmsvc.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Support Vector Regression.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">svmsvr = svm.SVR()</span><br><span class="line">svmsvr.fit(X_train,y_train)</span><br><span class="line">svmsvr.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Nearest Neighbors : 最近邻</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line">nbrs = NearestNeighbors(n_neighbors=<span class="number">2</span>, algorithm=<span class="string">'ball_tree'</span>).fit(X)</span><br><span class="line">distances, indices = nbrs.kneighbors(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">nkc = KNeighborsClassifier(<span class="number">15</span>, weights=<span class="string">'uniform'</span>)</span><br><span class="line">nkc.fit(X_train,y_train)</span><br><span class="line">nkc.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestCentroid</span><br><span class="line">clf = NearestCentroid(shrink_threshold=<span class="number">0.1</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test,y_test)</span><br></pre></td></tr></table></figure>
<h4 id="支持向量机svm">支持向量机（SVM）</h4>
<h5 id="简介">简介</h5>
<p>scikit-learn支持稠密的(dense)和稀疏的（sparse）数据，但是当测试数据是稀疏的时候，训练数据必须也是稀疏的。为了达到最优的性能，建议稠密数据使用<code>numpy.ndarray</code>,稀疏数据使用<code>scipy.sparse.csr_matrix</code> and <code>dtype=float64</code>.</p>
<h5 id="用途">用途</h5>
<ul>
<li>分类（classfication）</li>
<li>回归 (regression)</li>
<li>离群点检测 (outliers detection)</li>
</ul>
<h5 id="优点">优点</h5>
<ul>
<li>当特征维数很高时很有效(effective in high dimensional space)</li>
<li>当特征的维数远大于样本的数量的时候依然有效</li>
<li>使用的是训练点（training points）的子集进行决策函数（decision function）的计算，所以是内存高效的（memory efficient）</li>
<li>多种可供选择的核函数(kernel function)提高了算法的灵活性，核函数是可以根据自己的需要自定义的。</li>
</ul>
<h5 id="缺点">缺点</h5>
<ul>
<li>当特征的数量远大于样本的数量的时候，算法的性能会下降（poor performance）</li>
<li>SVM不直接提供概率估计（probability estimates），而是使用一个五折交叉验证（five-fold cross-validation）,计算复杂性较高，一般不适合海量数据的处理。</li>
</ul>
<h5 id="使用方法">使用方法</h5>
<h6 id="二分类">二分类</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#准备数据</span></span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment">#引入支持向量机</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">创建模型,这里有三种方法:</span></span><br><span class="line"><span class="string">svm.SVC(); svm.NuSVC(); svm.LinearSVC()</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">clf = svm.SVC()</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">训练数据，这里X是[n_samples,n_features],y是[n_labels]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">clf = clf.fit(X_train, y)</span><br><span class="line"><span class="comment">#使用训练好的模型预测</span></span><br><span class="line">y_predicted = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获得训练好的模型的一些参数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># get support vectors</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf.support_vectors_</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># get indices of support vectors</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf.support_</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>]...)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># get number of support vectors for each class</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf.n_support_</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>]...)</span><br><span class="line"><span class="comment">#get the params of the svm</span></span><br><span class="line">&gt;&gt;&gt;clf.coef_</span><br></pre></td></tr></table></figure>
<blockquote>
<p>上面是最简单的支持向量机的使用方式，下一步还需要了解可以设置的各个参数是什么意思，如何设置，如何交叉验证，如何选择和函数。</p>
</blockquote>
<h6 id="多分类">多分类</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]</span><br><span class="line">Y = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="comment"># "one-against-one"</span></span><br><span class="line">clf = svm.SVC(decision_function_shape=<span class="string">'ovo'</span>)</span><br><span class="line">clf.fit(X, Y)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">one-against-one 就是一对一，假设这四类的名称为a,b,c,d.</span></span><br><span class="line"><span class="string">则需要训练区分(a,b)(a,c)(a,d)(b,c)(b,d)(c,d)的6种模型，所以</span></span><br><span class="line"><span class="string">one-against-one这种策略在做多分类问题的时候会生成n*(n-1)/2个模型，每个模型区分其中的两个类。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">dec = clf.decision_function([[<span class="number">1</span>]])</span><br><span class="line">dec.shape[<span class="number">1</span>] <span class="comment"># 4 classes: 4*3/2 = 6</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> "one-vs-the-rest" 就是一对余下所有的，假设四类的名称为a,b,c,d;</span></span><br><span class="line"><span class="string"> 则需要训练区分(a,bcd),(b,acd)(c,abd)(d,abc)的4种模型，每个模型区分其中一个类，被除此类之外的所有类当作另外一个类处理。</span></span><br><span class="line"><span class="string"> 这种策略在做多分类问题的时候会生成n个模型。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">clf.decision_function_shape = <span class="string">"ovr"</span></span><br><span class="line">dec = clf.decision_function([[<span class="number">1</span>]])</span><br><span class="line">dec.shape[<span class="number">1</span>] <span class="comment"># 4 classes</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>一些补充说明：<code>SVC</code>和<code>NuSVC</code>实现了<code>one-against-one</code>(<code>ovo</code>)方法，<code>LinearSVC</code>实现了<code>one-vs-test</code>(<code>ovr</code>)和另外一个叫做<code>Crammer and Singer</code>的实现多分类的方法， 可以通过指定<code>multi_class='crammer_singer'</code>来使用它。不多实践证明，在使用<code>LinearSVC</code>的进行多分类的时候，优先选择<code>one-vs-test</code>(<code>ovr</code>)， 因为<code>one-vs-test</code>(<code>ovr</code>)和<code>crammer_singer</code>得到的结果差不多，但是前者的计算时间要短。</p>
</blockquote>
<h4 id="模型参数说明">模型参数说明</h4>
<h5 id="linearsvc">LinearSVC</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">clf = svm.LinearSVC()</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"><span class="keyword">print</span> clf</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">LinearSVC(C=<span class="number">1.0</span>, class_weight=<span class="literal">None</span>, dual=<span class="literal">True</span>, fit_intercept=<span class="literal">True</span>,</span><br><span class="line">     intercept_scaling=<span class="number">1</span>, loss=<span class="string">'squared_hinge'</span>, max_iter=<span class="number">1000</span>,</span><br><span class="line">     multi_class=<span class="string">'ovr'</span>, penalty=<span class="string">'l2'</span>, random_state=<span class="literal">None</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">     verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><strong>参数</strong> - <code>C</code>：可选参数，类型<code>float</code>,默认为1.0；误差的惩罚参数 - <code>class_weight</code>: 类型<code>dict</code>,可选参数，默认每个class的权重都是1.用来设置每个class的权重。 - <code>dual</code>:默认为<code>True</code>,类型<code>bool</code>,当<code>n_samples</code> &gt; <code>n_features</code>时，设置成<code>False</code>. - <code>fit_intercept</code>: 可选参数，类型为bool,默认为True. 意思是为模型计算截距（intercept），当数据事先已经是centered的时候，可以设置成False，不计算截距。 - <code>intercept_scaling</code>： 可选参数，类型为float,默认为1.意思是截距是否缩放。 - <code>loss</code>: 类型string,只能取“hinge” 和 “squared_hinge”,默认取“squared_hinge”；定义SVM的损失函数，“hinge”是标准的SVM损失函数，“squared_hinge”是标准损失函数的平方。 - <code>max_iter</code>： 类型为int,默认为1000，模型最大的迭代次数。 - <code>multi_class</code>：类型string，只能取’ovr’ 和 ‘crammer_singer’ (默认值是’ovr’)，当计算多分类的时候，指定多分类采取的策略。‘ovr’是将其中一类和剩下所有类二分，默认用这个策略就好。 - <code>penalty</code>： 类型string,只能取’l1’ or ‘l2’ (默认值是’l2’)，l1使参数稀疏，l2使大部分参数接近为0但是不是0，详细信息参考“机器学习中的范数” - <code>random_state</code>： 只能取int seed, RandomState instance, None 三个中的一个，默认值是None,指定产生伪随机数的时候使用的种子（seed） - <code>tol</code>：可选参数，类型为float,默认值是1e-4,指定停止时候的允许的误差。 - <code>verbose</code>：类型为int,默认值是0，是否开启详细的输出，默认不要开启就好。如果开启，在多线程的时候可能运行不正确。</p>
<p><strong>属性</strong> - <code>coef_</code>：训练好之后的SVM模型中的参数的取值（就是系数），当是二分类的时候，shape=[n_features],多分类的时候，shape = [n_classes,n_features] - <code>intercept_</code>:截距，二分类的时候shape = [1] ,多分类的时候shape=[n_classes]</p>
<h5 id="svc">SVC</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">clf = svm.SVC()</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"><span class="keyword">print</span> clf</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=<span class="literal">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</span><br><span class="line">  max_iter=<span class="number">-1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>, shrinking=<span class="literal">True</span>,</span><br><span class="line">  tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>参数</strong> - <code>C</code>=1.0, 可选参数，类型<code>float</code>,默认为1.0；误差的惩罚参数 - <code>cache_size</code>=200, 定义模型计算时使用的缓存大小，单位MB。 - <code>class_weight</code>=None,类型dict,默认为None,可以设置成’balanced’，这样会根据y自动计算每个class的权重。还可以手动设置每个class的权重。 - <code>coef0</code>=0.0,可选参数，类型为float,默认为0.0，独立于核函数（kernel function）的参数，只在’poly’ and ‘sigmoid’的时候有影响。 - <code>decision_function_shape</code>=None, ’ovo’, ‘ovr’ or None, default=None - <code>degree</code>=3, 可选参数，类型为int,默认为3，多项式和函数的度，其他类型的和函数自动忽略该参数。 - <code>gamma</code>=‘auto’, 可选参数，类型为float,默认为‘auto’,默认取 1/n_features作为gamma的值。 - <code>kernel</code>=‘rbf’,可选参数，类型为string，默认值为‘rbf’,定义SVM所使用的核函数，可选择的项如下： - linear - poly - rbf - sigmoid - precomputed - a callable(一个回调函数) - <code>max_iter</code>=-1, 最大迭代次数，默认为-1，意思是无限制。 - <code>probability</code>=False, 可选参数，类型bool,默认值为False. 是否进行概率估计，使用之前需要先调用fit方法。 - <code>random_state</code>=None, 只能取int seed, RandomState instance, None 三个中的一个，默认值是None,指定产生伪随机数的时候使用的种子（seed） - <code>shrinking</code>=True,可选参数，类型boolean,默认值为True,是否开启“shrinking heuristic” - <code>tol</code>=0.001, 可选参数，类型为float,默认值是1e-4,指定停止时候的允许的误差。 - <code>verbose</code>=False，类型为int,默认值是0，是否开启详细的输出，默认不要开启就好。如果开启，在多线程的时候可能运行不正确。</p>
<p><strong>属性</strong> - <code>support_</code> : array-like, shape = [n_SV]，支持向量的下标 - <code>n_support_</code> : array-like, dtype=int32, shape = [n_class] 每个类的支持向量的个数。 - <code>support_vectors_</code> ：shape = [n_SV, n_features]，支持向量(SVM确定了一个分类超平面，支持向量就是平移这个超平面，最先与数据集的交点。) - <code>dual_coef_</code> : array, shape = [n_class-1, n_SV] 在决策函数（decision function）中支持向量的系数 - <code>coef_</code> : array, shape = [n_class-1, n_features]，特征的权重，只在线性核的时候可用。 - <code>intercept_</code> : array, shape = [n_class * (n_class-1) / 2]，决策函数（decision function）中的常量。</p>
<h5 id="nusvc">NuSVC</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">clf = svm.NuSVC()</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"><span class="keyword">print</span> clf</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">NuSVC(cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">   decision_function_shape=<span class="literal">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</span><br><span class="line">   max_iter=<span class="number">-1</span>, nu=<span class="number">0.5</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">   shrinking=<span class="literal">True</span>, tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>参数</strong> 大部分都与<code>SVC</code>一样，只是使用了一个额外的参数控制支持向量（support vector）的个数。 - <code>nu</code>:可选参数，类型float，默认值是0.5，值必须要(0,1]之间。</p>
<p><strong>属性</strong> - <code>support_</code> : array-like, shape = [n_SV]，支持向量的下标 - <code>n_support_</code> : array-like, dtype=int32, shape = [n_class] 每个类的支持向量的个数。 - <code>support_vectors_</code> ：shape = [n_SV, n_features]，支持向量 - <code>dual_coef_</code> : array, shape = [n_class-1, n_SV] 在决策函数（decision function）中支持向量的系数 - <code>coef_</code> : array, shape = [n_class-1, n_features]，特征的权重，只在线性核的时候可用。 - <code>intercept_</code> : array, shape = [n_class * (n_class-1) / 2]，决策函数（decision function）中的常量。</p>
<h4 id="查看训练好的模型的参数">查看训练好的模型的参数</h4>
<h4 id="决策函数decision-function">决策函数（decision function）</h4>
<h4 id="核函数kernel-function">核函数（kernel function）</h4>
<p>优先使用‘rbf’调节参数，当特征的数量远远大于样本的数量的时候，考虑使用线性核函数。</p>
<hr>
<h4 id="随机梯度下降stochastic-gradient-descen">随机梯度下降（Stochastic Gradient Descen）</h4>
<p>分类，回归 ##### 简介 随机梯度下降法适用于特征数据大于10的5次方，样本数量大于10的5次方的大规模数据的处理领域。</p>
<h5 id="用途-1">用途</h5>
<p>可以处理大规模数据和稀疏数据。</p>
<h5 id="优点-1">优点</h5>
<ul>
<li>高效</li>
<li>易于实现</li>
</ul>
<h5 id="缺点-1">缺点</h5>
<ul>
<li>需要很多超参数</li>
<li>对特征的缩放敏感</li>
</ul>
<h5 id="使用方法-1">使用方法</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line">X = [[<span class="number">0.</span>, <span class="number">0.</span>], [<span class="number">1.</span>, <span class="number">1.</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">clf = SGDClassifier()</span><br><span class="line">clf.fit(X, y) <span class="comment">#训练</span></span><br><span class="line">clf.predict([[<span class="number">2.</span>, <span class="number">2.</span>]])  <span class="comment">#预测</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> clf</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">SGDClassifier(alpha=<span class="number">0.0001</span>, average=<span class="literal">False</span>, class_weight=<span class="literal">None</span>, epsilon=<span class="number">0.1</span>,</span><br><span class="line">       eta0=<span class="number">0.0</span>, fit_intercept=<span class="literal">True</span>, l1_ratio=<span class="number">0.15</span>,</span><br><span class="line">       learning_rate=<span class="string">'optimal'</span>, loss=<span class="string">'hinge'</span>, n_iter=<span class="number">5</span>, n_jobs=<span class="number">1</span>,</span><br><span class="line">       penalty=<span class="string">'l2'</span>, power_t=<span class="number">0.5</span>, random_state=<span class="literal">None</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">       verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;clf.coef_  <span class="comment">#模型系数</span></span><br><span class="line">&gt;&gt;&gt;Out[<span class="number">31</span>]: array([[ <span class="number">9.91080278</span>,  <span class="number">9.91080278</span>]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;clf.intercept_    <span class="comment">#截距</span></span><br><span class="line">&gt;&gt;&gt;array([<span class="number">-9.99002993</span>])</span><br></pre></td></tr></table></figure>
<p><strong>参数</strong> - <code>alpha</code>=0.0001, - <code>average</code>=False, - <code>class_weight</code>=None, epsilon=0.1, - <code>eta0</code>=0.0, - <code>fit_intercept</code>=True, - <code>l1_ratio</code>=0.15, - <code>learning_rate</code>=‘optimal’, - <code>loss</code>=‘hinge’, - <code>n_iter</code>=5, n_jobs=1, - <code>penalty</code>=‘l2’, - <code>power_t</code>=0.5, - <code>random_state</code>=None, - <code>shuffle</code>=True, - <code>verbose</code>=0, - <code>warm_start</code>=False</p>
<p><strong>属性</strong> - coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,n_features);Weights assigned to the features.</p>
<ul>
<li>intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,);Constants in decision function.</li>
</ul>
<h4 id="最近邻方法nearest-neighbors">最近邻方法（Nearest Neighbors）</h4>
<p>如果一个样本在特征空间中的k个最相 似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别.</p>
<h5 id="简介-1">简介</h5>
<p>scikit-learn实现了监督的和非监督的最近邻方法，决定最近邻的算法有<code>ball_tree</code>,<code>kd_tree</code>,<code>brute</code>,可以通过指定模型参数<code>algorithm</code>的值来指定到底使用哪一个算法。 主要功能是实现<strong><em>分类</em></strong>和<strong><em>回归</em></strong>。</p>
<h5 id="用法">用法</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line">X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">-3</span>, <span class="number">-2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line">nbrs = NearestNeighbors(n_neighbors=<span class="number">2</span>, algorithm=<span class="string">'ball_tree'</span>).fit(X)</span><br></pre></td></tr></table></figure>
<h3 id="模型持久化">模型持久化</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">clf_l1_LR = LogisticRegression(C=<span class="number">0.1</span>, penalty=<span class="string">'l1'</span>, tol=<span class="number">0.01</span>)</span><br><span class="line">joblib.dump(clf_l1_LR, <span class="string">'LogisticRegression.model'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="结果的可视化">结果的可视化</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">"VarianceThreshold For Feature Selection"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Number of features selected"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Cross validation score (nb of correct classifications)"</span>)</span><br><span class="line">plt.plot(lsvc_feature_num, lsvc_score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="数据预处理">数据预处理</h3>
<p><code>scikit-learn</code>提供了很多数据预处理的方法，使用的时候需要引入的包是<code>preprocessing</code>.</p>
<p><strong>缩放scale</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据最大值和最小值缩放到[0,1]范围</span></span><br><span class="line">min_max_scaler = MinMaxScaler()</span><br><span class="line">X_transformed = min_max_scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据标准化，使得均值为0，方差为1</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">X_transformed = ss.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 考虑离群点的缩放，首先排除离群点再缩放</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> RobustScaler</span><br><span class="line">robust_scaler = RobustScaler()</span><br><span class="line">X_transformed = robust_scaler.fit_transform(X)</span><br></pre></td></tr></table></figure>
<p><strong>one-hot编码</strong></p>
<p>对于离散的类别特征，可以使用<code>one-hot</code>编码来处理特征，这样处理之后的特征可以直接被一些学习器使用。该方法默认会根据类别的数量生成能够表示该类别的二进制编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">enc = OneHotEncoder()</span><br><span class="line">transformed_data = enc.transform(data).toarray()</span><br></pre></td></tr></table></figure>
<p><strong>特征组合</strong></p>
<p>特征组合的一个最简单的尝试是生成多项式特征，例如，如果有两个特征x_1,x_2,多项式为2的特征会自动生成1, x1,x2,x1*x2,x1<sup>2,x2</sup>2 这些特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">poly = PolynomialFeatures(<span class="number">2</span>)</span><br><span class="line">X_transformed = poly.fit_transform(X)</span><br></pre></td></tr></table></figure>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/hexoblog/2017/05/15/技术/机器学习/机器学习_时间序列预测の广告效果预测/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Plus récent</strong>
            <div class="article-nav-title">
                
                    机器学习_时间序列预测の广告效果预测
                
            </div>
        </a>
    
    
        <a href="/hexoblog/2017/05/14/技术/认知神经科学/fMRI相关的资源汇总/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Plus ancien</strong>
            <div class="article-nav-title">fMRI相关的资源汇总</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            FF120 &copy; 2019 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png"></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/hexoblog/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/hexoblog/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/hexoblog/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/hexoblog/js/main.js"></script>

    </div>
</body>
</html>